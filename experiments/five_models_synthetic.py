import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from autogluon.tabular import TabularPredictor
from autogluon.tabular.registry import ag_model_registry
# from custom_models.tabular_deepfm_torch_model import TabularDeepFMTorchModel  # ë°±ì—…ë¨
from custom_models.tabular_dcnv2_torch_model import TabularDCNv2TorchModel
from custom_models.tabular_dcnv2_fuxictr_torch_model_fixed import TabularDCNv2FuxiCTRTorchModel
from custom_models.focal_loss_implementation import CustomFocalDLModel
from custom_models.custom_nn_torch_model import CustomNNTorchModel
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from autogluon.common import space

# ëª¨ë¸ ë“±ë¡
# ag_model_registry.add(TabularDeepFMTorchModel)  # ë°±ì—…ë¨
ag_model_registry.add(TabularDCNv2TorchModel)
ag_model_registry.add(TabularDCNv2FuxiCTRTorchModel)
ag_model_registry.add(CustomFocalDLModel)
ag_model_registry.add(CustomNNTorchModel)

# í•©ì„± ë°ì´í„°ì…‹ ìƒì„±
print("=== í•©ì„± ë°ì´í„°ì…‹ ìƒì„± ===")
n_samples = 50000
minority_ratio = 0.0009   # 0.09%
majority_ratio = 1 - minority_ratio

X, y = make_classification(
    n_samples=n_samples,
    n_features=15,          # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°œìˆ˜
    n_informative=8,
    n_redundant=5,
    n_clusters_per_class=1,
    weights=[majority_ratio, minority_ratio],  # ë¶ˆê· í˜• ë¹„ìœ¨
    random_state=42
)

# DataFrame ë³€í™˜
df = pd.DataFrame(X, columns=[f"num_{i}" for i in range(X.shape[1])])
df["target"] = y

# --- ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€ ---
# ë¶„ìœ„ìˆ˜ ê¸°ë°˜ 4ê°œ ê·¸ë£¹
df["cat_1"] = pd.qcut(df["num_0"], q=4, labels=["A","B","C","D"])

# ì´ì§„ ë²”ì£¼í˜• (0ë³´ë‹¤ í°ì§€ ì—¬ë¶€)
df["cat_2"] = np.where(df["num_1"] > 0, "Yes", "No")

# 5ê°œ ê·¸ë£¹ ì¹´í…Œê³ ë¦¬
df["cat_3"] = pd.qcut(df["num_2"], q=5, labels=["L1","L2","L3","L4","L5"])

# --- ê²°ê³¼ í™•ì¸ ---
print("ë°ì´í„° í¬ê¸°:", df.shape)
print("í´ë˜ìŠ¤ ë¶„í¬:\n", df["target"].value_counts())
print("í´ë˜ìŠ¤ ë¹„ìœ¨:\n", df["target"].value_counts(normalize=True))
print("ë°ì´í„° ìƒ˜í”Œ:")
print(df.head())

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
train_data, test_data = train_test_split(df, test_size=0.2, stratify=df["target"], random_state=42)

print(f"\ní•™ìŠµ ë°ì´í„° í¬ê¸°: {train_data.shape}")
print(f"í•™ìŠµ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬:\n{train_data['target'].value_counts()}")
print(f"í•™ìŠµ ë°ì´í„° í´ë˜ìŠ¤ ë¹„ìœ¨:\n{train_data['target'].value_counts(normalize=True)}")

print(f"\ní…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_data.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬:\n{test_data['target'].value_counts()}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤ ë¹„ìœ¨:\n{test_data['target'].value_counts(normalize=True)}")

print("\n=== 5ê°œ ëª¨ë¸ ë™ì‹œ í•™ìŠµ (DCNv2, DCNv2_FUXICTR, CustomFocalDLModel, RandomForest, CustomNNTorchModel) ===")

# 5ê°œ ëª¨ë¸ì„ í•œ ë²ˆì— í•™ìŠµ (AutoGluon ìë™ holdout ë¶„í•  ì‚¬ìš©)
predictor = TabularPredictor(label="target", problem_type="binary", eval_metric="f1", path="models/five_models_synthetic").fit(
    train_data,
    hyperparameters={
        # DCNv2 - ìˆ˜ì¹˜í˜• íŠ¹ì„±ì— ì í•©í•œ ëª¨ë¸ (ìµœê³  ì„±ëŠ¥ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©)
        "DCNV2": {
            "num_cross_layers": 2,
            "cross_dropout": 0.1181,
            "low_rank": 29,
            "deep_output_size": 98,
            "deep_hidden_size": 91,
            "deep_dropout": 0.1583,
            "deep_layers": 3,
            'epochs_wo_improve': 5,
            'num_epochs': 20,
            "lr_scheduler": True,
            "scheduler_type": "cosine",
            "lr_scheduler_min_lr": 1e-6,
            "learning_rate": 0.000629,
            "weight_decay": 5.68e-12,
            "dropout_prob": 0.5,
            "activation": "relu",
            "optimizer": "adam",
            "hidden_size": 128,
            "use_batchnorm": True,
        },
        # FuxiCTR DCNv2 - MoE êµ¬ì¡°ì˜ DCNv2 (ìµœê³  ì„±ëŠ¥ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©)
        "DCNV2_FUXICTR": {
            "num_cross_layers": 2,
            "cross_dropout": 0.1181,
            "low_rank": 29,
            "deep_output_size": 98,
            "deep_hidden_size": 91,
            "deep_dropout": 0.1583,
            "deep_layers": 3,
            'epochs_wo_improve': 5,
            'num_epochs': 20,
            "lr_scheduler": True,
            "scheduler_type": "cosine",
            "lr_scheduler_min_lr": 1e-6,
            "learning_rate": 0.000629,
            "weight_decay": 5.68e-12,
            "dropout_prob": 0.5,
            "activation": "relu",
            "optimizer": "adam",
            "hidden_size": 128,
            "use_batchnorm": True,
            # FuxiCTR íŠ¹í™” íŒŒë¼ë¯¸í„°
            "use_low_rank_mixture": True,
            "num_experts": 4,
            "model_structure": "parallel",
        },
        # CustomFocalDLModel (Focal Loss ì‚¬ìš©) - Cosine Annealing ìŠ¤ì¼€ì¤„ëŸ¬ í¬í•¨
        'CUSTOM_FOCAL_DL': [{
            'max_batch_size': 512,
            'num_epochs': 20,
            'epochs_wo_improve': 5,
            'optimizer': 'adam',
            'learning_rate': 0.0008,  # Focal Lossì— ì í•©í•œ LR
            'weight_decay': 0.0001,
            'dropout_prob': 0.1,
            'num_layers': 4,
            'hidden_size': 128,
            'activation': 'relu',
            'lr_scheduler': True,
            'scheduler_type': 'cosine',
            'lr_scheduler_min_lr': 1e-6,
        }],
        # RandomForest - ê¸°ë³¸ ì„¤ì •
        "RF": {
            "n_estimators": 100,
            "max_depth": 10,
            "min_samples_split": 5,
            "min_samples_leaf": 2,
            "criterion": "gini",
        },
        # CustomNNTorchModel (ì¼ë°˜ CrossEntropy) - ìŠ¤ì¼€ì¤„ëŸ¬ í¬í•¨
        'CUSTOM_NN_TORCH': [{
            'max_batch_size': 512,
            'num_epochs': 20,
            'epochs_wo_improve': 5,
            'optimizer': 'adam',
            'learning_rate': 0.0005,  # ë” ì•ˆì •ì ì¸ LR
            'weight_decay': 0.0001,
            'dropout_prob': 0.1,
            'num_layers': 4,
            'hidden_size': 128,
            'activation': 'relu',
            'lr_scheduler': True,
            'scheduler_type': 'cosine',
            'lr_scheduler_min_lr': 1e-6,
        }],
    },
    time_limit=900,  # 15ë¶„
    verbosity=4,  # ìµœê³  verbosity (ê°€ì¥ ìì„¸í•œ ë¡œê·¸)
)

print("\n=== í•™ìŠµ ì™„ë£Œ! ê²°ê³¼ í™•ì¸ ===")
print("ë¦¬ë”ë³´ë“œ (ê²€ì¦ ë°ì´í„° ê¸°ì¤€):")
print(predictor.leaderboard())

print("\n=== ë¦¬ë”ë³´ë“œ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ì¤€) ===")
print(predictor.leaderboard(data=test_data))

print("\n=== ëª¨ë¸ë³„ ìƒì„¸ ì •ë³´ (ê²€ì¦ ë°ì´í„° ê¸°ì¤€) ===")
leaderboard_val = predictor.leaderboard()
for idx, row in leaderboard_val.iterrows():
    model_name = row['model']
    score = row['score_val']
    fit_time = row['fit_time_marginal']
    print(f"\n{model_name}:")
    print(f"  - ê²€ì¦ ì„±ëŠ¥ ì ìˆ˜: {score:.4f}")
    print(f"  - í•™ìŠµ ì‹œê°„: {fit_time:.2f}ì´ˆ")

print("\n=== ëª¨ë¸ë³„ ìƒì„¸ ì •ë³´ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ì¤€) ===")
leaderboard_test = predictor.leaderboard(data=test_data)
for idx, row in leaderboard_test.iterrows():
    model_name = row['model']
    score = row['score_val']
    print(f"\n{model_name}:")
    print(f"  - í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì ìˆ˜: {score:.4f}")

print("\n=== ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (ê²€ì¦ ë°ì´í„° ê¸°ì¤€) ===")
best_row_val = leaderboard_val.loc[leaderboard_val['score_val'].idxmax()]
best_model_val = best_row_val['model']
print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_val}")
print(f"ê²€ì¦ ì„±ëŠ¥ ì ìˆ˜: {best_row_val['score_val']:.4f}")

print("\n=== ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ì¤€) ===")
best_row_test = leaderboard_test.loc[leaderboard_test['score_val'].idxmax()]
best_model_test = best_row_test['model']
print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_test}")
print(f"í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì ìˆ˜: {best_row_test['score_val']:.4f}")

print("\n=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ê²€ì¦ ë°ì´í„° ê¸°ì¤€) ===")
print("ëª¨ë¸ë³„ ì„±ëŠ¥ ìˆœìœ„:")
for idx, row in leaderboard_val.iterrows():
    if 'WeightedEnsemble' not in row['model']:  # ì•™ìƒë¸” ì œì™¸í•˜ê³  ê°œë³„ ëª¨ë¸ë§Œ
        print(f"{idx+1}. {row['model']}: F1 = {row['score_val']:.4f}")

print("\n=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ì¤€) ===")
print("ëª¨ë¸ë³„ ì„±ëŠ¥ ìˆœìœ„:")
for idx, row in leaderboard_test.iterrows():
    if 'WeightedEnsemble' not in row['model']:  # ì•™ìƒë¸” ì œì™¸í•˜ê³  ê°œë³„ ëª¨ë¸ë§Œ
        print(f"{idx+1}. {row['model']}: F1 = {row['score_val']:.4f}")

print("\n=== Focal Loss vs ì¼ë°˜ CrossEntropy ë¹„êµ (ê²€ì¦ ë°ì´í„° ê¸°ì¤€) ===")
focal_score_val = None
nn_torch_score_val = None

for idx, row in leaderboard_val.iterrows():
    if 'CUSTOM_FOCAL_DL' in row['model']:
        focal_score_val = row['score_val']
    elif 'CUSTOM_NN_TORCH' in row['model']:
        nn_torch_score_val = row['score_val']

if focal_score_val is not None and nn_torch_score_val is not None:
    print(f"CUSTOM_FOCAL_DL (Focal Loss): {focal_score_val:.4f}")
    print(f"CUSTOM_NN_TORCH (ì¼ë°˜ CrossEntropy): {nn_torch_score_val:.4f}")
    if focal_score_val > nn_torch_score_val:
        print("âœ… Focal Lossê°€ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤!")
    elif focal_score_val < nn_torch_score_val:
        print("âŒ ì¼ë°˜ CrossEntropyê°€ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ¤ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë™ì¼í•©ë‹ˆë‹¤.")
else:
    print("Focal Lossì™€ ì¼ë°˜ CrossEntropy ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

print("\n=== Focal Loss vs ì¼ë°˜ CrossEntropy ë¹„êµ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ì¤€) ===")
focal_score_test = None
nn_torch_score_test = None

for idx, row in leaderboard_test.iterrows():
    if 'CUSTOM_FOCAL_DL' in row['model']:
        focal_score_test = row['score_val']
    elif 'CUSTOM_NN_TORCH' in row['model']:
        nn_torch_score_test = row['score_val']

if focal_score_test is not None and nn_torch_score_test is not None:
    print(f"CUSTOM_FOCAL_DL (Focal Loss): {focal_score_test:.4f}")
    print(f"CUSTOM_NN_TORCH (ì¼ë°˜ CrossEntropy): {nn_torch_score_test:.4f}")
    if focal_score_test > nn_torch_score_test:
        print("âœ… Focal Lossê°€ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤!")
    elif focal_score_test < nn_torch_score_test:
        print("âŒ ì¼ë°˜ CrossEntropyê°€ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ¤ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë™ì¼í•©ë‹ˆë‹¤.")
else:
    print("Focal Lossì™€ ì¼ë°˜ CrossEntropy ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

print("\n=== ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
predictions = predictor.predict(test_data)
probabilities = predictor.predict_proba(test_data)

print(f"ì˜ˆì¸¡ ê²°ê³¼ í˜•íƒœ: {predictions.shape}")
print(f"í™•ë¥  ì˜ˆì¸¡ í˜•íƒœ: {probabilities.shape}")
print(f"ì˜ˆì¸¡ê°’ ìƒ˜í”Œ: {predictions.head()}")
print(f"í™•ë¥ ê°’ ìƒ˜í”Œ:\n{probabilities.head()}")

print("\n=== í•©ì„± ë°ì´í„°ì…‹ìœ¼ë¡œ 5ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ===") 