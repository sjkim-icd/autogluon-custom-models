# [my_models] DCNv2 Paper ì§„ìž…ì  - ë…¼ë¬¸ê³¼ ë™ì¼í•œ êµ¬í˜„

from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel
from autogluon.common import space
from autogluon.tabular.models.tabular_nn.hyperparameters.parameters import get_default_param
from autogluon.tabular.models.tabular_nn.hyperparameters.searchspaces import get_default_searchspace
from custom_models.dcnv2_block_paper import DCNv2NetPaper
import os

class TabularDCNv2PaperTorchModel(TabularNeuralNetTorchModel):
    ag_key = "DCNV2_PAPER"         # â† ìƒˆë¡œìš´ í‚¤
    ag_name = "DCNV2_PAPER"        # â† ìƒˆë¡œìš´ ì´ë¦„
    ag_priority = 100
    _model_name = "TabularDCNv2PaperTorchModel"
    _model_type = "tabular_dcnv2_paper_torch_model"
    _typestr = "tabular_dcnv2_paper_torch_model_v1_paper"  # âœ… ìƒˆë¡œìš´ íƒ€ìž…

    @classmethod
    def register(cls):
        """ëª¨ë¸ì„ AutoGluonì— ë“±ë¡"""
        from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import ag_model_registry
        ag_model_registry.add(cls)
        print(f"âœ… {cls.ag_name} ëª¨ë¸ì´ AutoGluonì— ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")

    def __init__(self, **kwargs):
        print("ðŸ”§ DCNv2 Paper __init__() í˜¸ì¶œë¨!")
        print(f"ðŸ“‹ ë°›ì€ kwargs: {list(kwargs.keys())}")
        super().__init__(**kwargs)
        print("âœ… DCNv2 Paper ì´ˆê¸°í™” ì™„ë£Œ!")

    def _set_default_params(self):
        """ë…¼ë¬¸ê³¼ ë™ì¼í•œ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •"""
        print("ðŸ”§ DCNv2 Paper _set_default_params() í˜¸ì¶œë¨!")
        
        # AutoGluon ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        default_params = get_default_param(problem_type=self.problem_type, framework="pytorch")
        
        # ë…¼ë¬¸ê³¼ ë™ì¼í•œ DCNv2 íŒŒë¼ë¯¸í„° (ë‹¨ìˆœí™”)
        dcnv2_paper_params = {
            'num_cross_layers': 2,  # ë…¼ë¬¸ ê¸°ë³¸ê°’
            'cross_dropout': 0.1,
            'low_rank': 32,  # ë…¼ë¬¸ ê¸°ë³¸ê°’
            'deep_output_size': 128,
            'deep_hidden_size': 128,
            'deep_dropout': 0.1,
            'deep_layers': 3,
            # Learning Rate Scheduler ì„¤ì •
            'lr_scheduler': True,
            'scheduler_type': 'plateau',
            'lr_scheduler_patience': 5,
            'lr_scheduler_factor': 0.2,
            'lr_scheduler_min_lr': 1e-6,
        }
        default_params.update(dcnv2_paper_params)
        
        for param, val in default_params.items():
            self._set_default_param_value(param, val)
        
        print(f"âœ… DCNv2 Paper ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ: {list(default_params.keys())}")

    def _get_default_searchspace(self):
        """ë…¼ë¬¸ê³¼ ë™ì¼í•œ ê²€ìƒ‰ ê³µê°„ ì •ì˜ (ë‹¨ìˆœí™”)"""
        print("ðŸ” DCNv2 Paper _get_default_searchspace() í˜¸ì¶œë¨!")
        
        # AutoGluon ê¸°ë³¸ Search Space ê°€ì ¸ì˜¤ê¸°
        base_searchspace = get_default_searchspace(problem_type=self.problem_type, framework="pytorch")
        
        # ë…¼ë¬¸ê³¼ ë™ì¼í•œ DCNv2 Search Space (ë‹¨ìˆœí™”)
        dcnv2_paper_searchspace = {
            'num_cross_layers': space.Categorical(2, 3),  # ë…¼ë¬¸ ë²”ìœ„
            'cross_dropout': space.Categorical(0.0, 0.1),
            'low_rank': space.Categorical(16, 32, 64),  # ë…¼ë¬¸ ë²”ìœ„
            'deep_output_size': space.Categorical(64, 128),
            'deep_hidden_size': space.Categorical(64, 128),
            'deep_dropout': space.Categorical(0.1, 0.2),
            'deep_layers': space.Categorical(2, 3),
            # Learning Rate Scheduler Search Space
            'lr_scheduler': space.Categorical(True, False),
            'scheduler_type': space.Categorical('plateau', 'cosine'),
            'lr_scheduler_patience': space.Categorical(3, 5),
            'lr_scheduler_factor': space.Categorical(0.1, 0.2),
            'lr_scheduler_min_lr': space.Real(1e-7, 1e-5, default=1e-6, log=True),
            # fitì—ì„œ ë„˜ê¸°ëŠ” ê³ ì • íŒŒë¼ë¯¸í„°ë„ í¬í•¨
            'epochs_wo_improve': space.Categorical(5),
            'num_epochs': space.Categorical(20),
        }
        
        # ê¸°ë³¸ Search Spaceì™€ ì»¤ìŠ¤í…€ Search Space í•©ì¹˜ê¸°
        base_searchspace.update(dcnv2_paper_searchspace)
        
        print(f"âœ… DCNv2 Paper ê²€ìƒ‰ ê³µê°„ ìƒì„±ë¨: {list(base_searchspace.keys())}")
        return base_searchspace

    @classmethod
    def get_default_searchspace(cls, problem_type, num_classes=None, **kwargs):
        """ë…¼ë¬¸ê³¼ ë™ì¼í•œ ê²€ìƒ‰ ê³µê°„ ì •ì˜ (í´ëž˜ìŠ¤ ë©”ì„œë“œ - ì™¸ë¶€ í˜¸ì¶œìš©)"""
        # AutoGluon ê¸°ë³¸ Search Space ê°€ì ¸ì˜¤ê¸°
        base_searchspace = get_default_searchspace(problem_type=problem_type, framework="pytorch")
        
        # ë…¼ë¬¸ê³¼ ë™ì¼í•œ DCNv2 Search Space (ë‹¨ìˆœí™”)
        dcnv2_paper_searchspace = {
            'num_cross_layers': space.Categorical(2, 3),
            'learning_rate': space.Categorical(0.0001, 0.001),
            'epochs_wo_improve': space.Categorical(5),
            'num_epochs': space.Categorical(20),
        }
        
        # ê¸°ë³¸ Search Spaceì™€ ì»¤ìŠ¤í…€ Search Space í•©ì¹˜ê¸°
        base_searchspace.update(dcnv2_paper_searchspace)
        return base_searchspace

    def _get_net(self, train_dataset, params):
        print("ðŸ”§ DCNv2 Paper _get_net() í˜¸ì¶œë¨!")
        print(f"ðŸ“‹ ë°›ì€ íŒŒë¼ë¯¸í„°: {list(params.keys())}")
        print(f"ðŸ“Š íŒŒë¼ë¯¸í„° íƒ€ìž… í™•ì¸:")
        for key, value in params.items():
            print(f"  {key}: {type(value)} = {value}")
        
        # ë…¼ë¬¸ê³¼ ë™ì¼í•œ DCNv2Net ìƒì„±
        params = self._set_net_defaults(train_dataset, params)
        
        # DCNv2NetPaper ìƒì„± - ë…¼ë¬¸ê³¼ ë™ì¼í•œ êµ¬í˜„
        model = DCNv2NetPaper(
            problem_type=self.problem_type,
            num_net_outputs=self._get_num_net_outputs(),
            quantile_levels=self.quantile_levels,
            train_dataset=train_dataset,
            device=self.device,
            **params,  # ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
        )
        model = model.to(self.device)
        
        # self.model ì„¤ì •
        self.model = model
        
        if not os.path.exists(self.path):
            os.makedirs(self.path)
        
        print("âœ… DCNv2NetPaper ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
        return model
    
    def _train_net(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset=None, test_dataset=None, time_limit=None, reporter=None, verbosity=2):
        """ë…¼ë¬¸ê³¼ ë™ì¼í•œ í•™ìŠµ ë©”ì„œë“œ"""
        print("ðŸš€ DCNv2 Paper _train_net í˜¸ì¶œë¨!")
        import torch
        import torch.optim.lr_scheduler as lr_scheduler
        
        # LR ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        scheduler = None
        if hasattr(self.model, 'lr_scheduler') and self.model.lr_scheduler:
            if self.model.scheduler_type == 'cosine':
                scheduler = lr_scheduler.CosineAnnealingLR(
                    self.optimizer, 
                    T_max=num_epochs,
                    eta_min=self.model.lr_scheduler_min_lr
                )
                print(f"âœ… DCNv2 Paper: Cosine Annealing LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨ (min_lr={self.model.lr_scheduler_min_lr})")
            elif self.model.scheduler_type == 'plateau':
                scheduler = lr_scheduler.ReduceLROnPlateau(
                    self.optimizer,
                    mode='max',
                    factor=self.model.lr_scheduler_factor,
                    patience=self.model.lr_scheduler_patience,
                    min_lr=self.model.lr_scheduler_min_lr
                )
                print(f"âœ… DCNv2 Paper: ReduceLROnPlateau LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨ (factor={self.model.lr_scheduler_factor}, patience={self.model.lr_scheduler_patience})")
            else:
                print(f"âš ï¸ DCNv2 Paper: ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ìž… '{self.model.scheduler_type}'")
        
        # ë¶€ëª¨ í´ëž˜ìŠ¤ì˜ ê¸°ë³¸ í•™ìŠµ ë¡œì§ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í•¨ê»˜)
        if scheduler:
            # ì»¤ìŠ¤í…€ í•™ìŠµ ë£¨í”„ë¡œ LR ëª¨ë‹ˆí„°ë§
            self._train_with_scheduler(train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity, scheduler)
        else:
            # ê¸°ë³¸ í•™ìŠµ
            super()._train_net(train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity)
    
    def _train_with_scheduler(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset=None, test_dataset=None, time_limit=None, reporter=None, verbosity=2, scheduler=None):
        """ë…¼ë¬¸ê³¼ ë™ì¼í•œ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í•¨ê»˜ í•™ìŠµ"""
        import torch
        import time
        import io
        
        # ê¸°ë³¸ ì„¤ì •
        self.model.init_params()
        train_dataloader = train_dataset.build_loader(batch_size, self.num_dataloading_workers, is_test=False)
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get("loss_function", "auto") == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()
        
        # Early stopping ì„¤ì •
        if epochs_wo_improve is not None:
            early_stopping_method = self._get_early_stopping_strategy(num_rows_train=len(train_dataset))
        else:
            early_stopping_method = self._get_early_stopping_strategy(num_rows_train=len(train_dataset))
        
        # Validation ë°ì´í„° ì¤€ë¹„
        if val_dataset is not None:
            y_val = val_dataset.get_labels()
            if y_val.ndim == 2 and y_val.shape[1] == 1:
                y_val = y_val.flatten()
        else:
            y_val = None
        
        # ëª¨ë¸ ì €ìž¥ìš© ë²„í¼
        io_buffer = None
        best_epoch = 0
        best_val_metric = -float('inf')
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            num_batches = 0
            
            # í˜„ìž¬ LR ì¶œë ¥
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ë°°ì¹˜ë³„ í•™ìŠµ
            for batch_idx, data_batch in enumerate(train_dataloader):
                self.optimizer.zero_grad()
                loss = self.model.compute_loss(data_batch, **loss_kwargs)
                loss.backward()
                
                # Gradient clipping ì¶”ê°€ (nan ë°©ì§€)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            # ì—í¬í¬ í‰ê·  ì†ì‹¤
            avg_loss = total_loss / num_batches
            epoch_time = time.time() - epoch_start_time
            
            # Validation í‰ê°€
            val_metric = None
            if val_dataset is not None:
                val_metric = self.score(X=val_dataset, y=y_val, metric=self.stopping_metric, _reset_threads=False)
                
                # Best model ì €ìž¥
                if val_metric > best_val_metric:
                    best_val_metric = val_metric
                    io_buffer = io.BytesIO()
                    torch.save(self.model, io_buffer)
                    best_epoch = epoch + 1
            
            # ê°„ë‹¨í•œ epoch ë¡œê·¸ ì¶œë ¥
            import time as time_module
            current_time = time_module.strftime("%Y-%m-%d %H:%M:%S")
            
            if val_metric is not None:
                log_msg = f"[{current_time}] Epoch {epoch+1}/{num_epochs}: " \
                          f"Train loss: {avg_loss:.4f}, " \
                          f"Val {self.stopping_metric.name}: {val_metric:.4f}, " \
                          f"LR: {current_lr:.2e}, " \
                          f"Best Epoch: {best_epoch}, " \
                          f"Time: {epoch_time:.2f}s"
            else:
                log_msg = f"[{current_time}] Epoch {epoch+1}/{num_epochs}: " \
                          f"Train loss: {avg_loss:.4f}, " \
                          f"LR: {current_lr:.2e}, " \
                          f"Time: {epoch_time:.2f}s"
            
            print(log_msg)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            if scheduler:
                if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingLR):
                    scheduler.step()
                elif isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    scheduler.step(val_metric if val_metric is not None else avg_loss)
            
            # Early stopping ì²´í¬
            if val_dataset is not None:
                is_best = (val_metric > best_val_metric) if epoch > 0 else True
                early_stop = early_stopping_method.update(cur_round=epoch, is_best=is_best)
                if early_stop:
                    print(f"   Early stopping at epoch {epoch+1}")
                    break
        
        # Best model ë¡œë“œ
        if io_buffer is not None:
            io_buffer.seek(0)
            self.model = torch.load(io_buffer, weights_only=False)
            print(f"   Best model loaded from epoch {best_epoch} (Val f1: {best_val_metric:.4f})") 