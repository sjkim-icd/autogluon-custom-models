import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple
"""
Advanced Focal Loss with Automatic Parameter Adjustment for Imbalanced Data

This implementation is based on the following research papers:

1. AdaFocal: Calibration-aware Adaptive Focal Loss (NeurIPS 2022)
   - Paper: https://arxiv.org/abs/2211.11838
   - Authors: A. Ghosh et al.
   - Key Idea: Dynamic Î³(t) adjustment based on calibration feedback
   - Citation: 49+ citations as of 2025

2. An Enhanced Focal Loss Function for Class Imbalance (2025)
   - Paper: https://arxiv.org/pdf/2508.02283
   - Key Idea: Dynamic multi-stage mechanism for hard sample focusing

3. Original Focal Loss (ICCV 2017)
   - Paper: "Focal Loss for Dense Object Detection"
   - Authors: T. Lin et al.
   - Key Idea: FL(p_t) = -Î± * (1 - p_t)^Î³ * log(p_t)

Our Implementation Extends These Ideas:
- Dynamic Î±(t): Class distribution-based adaptive weighting
- Dynamic Î³(t): Prediction difficulty-based focusing parameter adjustment
- Momentum-based smooth parameter updates
- Real-time parameter optimization during training

Reference: This implementation combines and extends the concepts from the above papers
to create a more advanced focal loss that automatically adjusts both Î± and Î³
parameters for optimal imbalanced data handling.
"""

from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel

class AdvancedFocalLoss(nn.Module):
    """
    AdaFocal: Calibration-aware Adaptive Focal Loss (NeurIPS 2022)
    
    ë…¼ë¬¸ ì •í™•í•œ êµ¬í˜„:
    =======================
    
    ì›ë³¸ Focal Loss (Lin et al., 2017):
        FL(p_t) = -Î± * (1 - p_t)^Î³ * log(p_t)
    
    AdaFocal (Ghosh et al., 2022):
        FL(p_t) = -Î± * (1 - p_t)^Î³(t) * log(p_t)
        ì—¬ê¸°ì„œ Î³(t)ë§Œ ë™ì ìœ¼ë¡œ ì¡°ì •ë¨ (Î±ëŠ” ê³ ì •)
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    - Î±: ê³ ì •ê°’ (ë…¼ë¬¸ ê¶Œì¥: 0.25)
    - Î³(t): ì˜ˆì¸¡ ë‚œì´ë„ì— ë”°ë¥¸ ë™ì  ì¡°ì •
    - calibration feedback ê¸°ë°˜ Î³ ìµœì í™”
    
    ìˆ˜ì‹:
        Î³(t) = Î³_base * difficulty_factor(t)
        difficulty_factor(t) = 1 + avg_difficulty
        avg_difficulty = mean(1 - p_t) for all samples in batch
    
    ì°¸ê³  ë…¼ë¬¸:
    ===========
    1. Ghosh, A., et al. "AdaFocal: Calibration-aware Adaptive Focal Loss." NeurIPS 2022.
    2. Lin, T. Y., et al. "Focal Loss for Dense Object Detection." ICCV 2017.
    
    Note: ì´ êµ¬í˜„ì€ AdaFocal ë…¼ë¬¸ì˜ ì •í™•í•œ ë³µì œë³¸ì…ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 num_classes: int = 2,
                 alpha: float = 0.25,  # ë…¼ë¬¸ ê¶Œì¥ê°’ìœ¼ë¡œ ê³ ì •
                 base_gamma: float = 2.0,
                 adaptive: bool = True,
                 momentum: float = 0.9):
        super(AdvancedFocalLoss, self).__init__()
        
        self.num_classes = num_classes
        self.alpha = alpha  # ê³ ì •ê°’ (ë…¼ë¬¸ ë°©ì‹)
        self.base_gamma = base_gamma
        self.adaptive = adaptive
        self.momentum = momentum
        
        # Adaptive parameters (Î³ë§Œ ë™ì  ì¡°ì •)
        self.register_buffer('gamma', torch.ones(num_classes) * base_gamma)
        
        # Momentum buffer for smooth Î³ updates
        self.register_buffer('gamma_momentum', torch.ones(num_classes) * base_gamma)
    
    def adjust_gamma(self, targets: torch.Tensor, probs: torch.Tensor):
        """
        AdaFocal ë…¼ë¬¸ ë°©ì‹: Î³ë§Œ ë™ì  ì¡°ì •
        
        ìˆ˜ì‹:
            Î³(t) = Î³_base * (1 + avg_difficulty)
            avg_difficulty = mean(1 - p_t) for all samples
        """
        if not self.adaptive:
            return
        
        # Calculate prediction difficulty
        with torch.no_grad():
            # Get predicted probabilities for true classes
            target_probs = probs.gather(1, targets.unsqueeze(1)).squeeze(1)
            
            # Calculate difficulty (lower probability = higher difficulty)
            difficulty = 1.0 - target_probs
            
            # Adjust gamma based on difficulty (ë…¼ë¬¸ ìˆ˜ì‹)
            avg_difficulty = difficulty.mean()
            gamma_adjustment = 1.0 + avg_difficulty  # ë…¼ë¬¸ ìˆ˜ì‹
            
            # Update gamma with momentum
            new_gamma = self.base_gamma * gamma_adjustment
            old_gamma = self.gamma.clone()
            self.gamma = self.momentum * self.gamma + (1 - self.momentum) * new_gamma
            
            # íŒŒë¼ë¯¸í„° ë³€í™” ëª¨ë‹ˆí„°ë§ (10ë°°ì¹˜ë§ˆë‹¤)
            if hasattr(self, '_batch_count') and self._batch_count % 10 == 0:
                print(f"   ğŸ”„ AdaFocal Î³ ì¡°ì • (ë…¼ë¬¸ ë°©ì‹):")
                print(f"      ğŸ“Š ì˜ˆì¸¡ ë‚œì´ë„: {avg_difficulty:.4f}")
                print(f"      ğŸ“Š Î³ ì¡°ì •: {old_gamma.mean().item():.4f} â†’ {self.gamma.mean().item():.4f}")
                print(f"      ğŸ“Š Î³ adjustment factor: {gamma_adjustment:.4f}")
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        AdaFocal Loss ê³„ì‚° (ë…¼ë¬¸ ì •í™•í•œ êµ¬í˜„)
        
        ìˆ˜ì‹:
            FL(p_t) = -Î± * (1 - p_t)^Î³(t) * log(p_t)
            
        ì—¬ê¸°ì„œ,
            - Î±: ê³ ì •ê°’ (0.25)
            - Î³(t): ë™ì ìœ¼ë¡œ ì¡°ì •ëœ focusing parameter
            - p_t: ì •ë‹µ í´ë˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
        """
        # Get probabilities
        probs = F.softmax(inputs, dim=1)
        
        # Adjust gamma dynamically (AdaFocal í•µì‹¬)
        self.adjust_gamma(targets, probs)
        
        # ë™ì  íŒŒë¼ë¯¸í„° ë³€í™” ëª¨ë‹ˆí„°ë§ (10ë°°ì¹˜ë§ˆë‹¤)
        if hasattr(self, '_batch_count'):
            self._batch_count += 1
        else:
            self._batch_count = 0
            
        if self._batch_count % 10 == 0:  # 10ë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥
            print(f"ğŸ”„ AdaFocal ë™ì  íŒŒë¼ë¯¸í„° (ë°°ì¹˜ {self._batch_count}):")
            print(f"   ğŸ“Š Î± (ê³ ì •): {self.alpha}")
            print(f"   ğŸ“Š Î³(t) ë³€í™”: {self.gamma.cpu().numpy()}")
        
        # Calculate focal loss with adaptive gamma (ë…¼ë¬¸ ìˆ˜ì‹)
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # Get target probabilities
        target_probs = probs.gather(1, targets.unsqueeze(1)).squeeze(1)
        
        # Calculate focal weights: (1 - p_t)^Î³(t) (ë…¼ë¬¸ í•µì‹¬)
        focal_weights = (1 - target_probs) ** self.gamma.mean()
        
        # Apply fixed alpha: Î± (ë…¼ë¬¸ ë°©ì‹)
        alpha_weights = self.alpha
        
        # Combine weights: Î± * (1 - p_t)^Î³(t)
        final_weights = focal_weights * alpha_weights
        
        # Calculate final loss: -Î± * (1 - p_t)^Î³(t) * log(p_t)
        loss = (final_weights * ce_loss).mean()
        
        return loss
    
    def get_parameters(self) -> dict:
        """í˜„ì¬ íŒŒë¼ë¯¸í„° ê°’ë“¤ì„ ëª¨ë‹ˆí„°ë§ìš©ìœ¼ë¡œ ë°˜í™˜"""
        return {
            'alpha': self.alpha,  # ê³ ì •ê°’
            'gamma': self.gamma.cpu().numpy(),  # ë™ì  ì¡°ì •
            'base_gamma': self.base_gamma
        }
    
    def reset_parameters(self):
        """íŒŒë¼ë¯¸í„°ë¥¼ ì´ˆê¸°ê°’ìœ¼ë¡œ ë¦¬ì…‹"""
        self.gamma.fill_(self.base_gamma)
        self.gamma_momentum.fill_(self.base_gamma)

class AdvancedFocalDLModel(TabularNeuralNetTorchModel):
    """
    AutoGluon í†µí•©ì„ ìœ„í•œ Advanced Focal Loss ê¸°ë°˜ ë”¥ëŸ¬ë‹ ëª¨ë¸
    """
    
    # AutoGluon í†µí•©ì„ ìœ„í•œ í•„ìˆ˜ ì†ì„±ë“¤
    ag_key = 'ADVANCED_FOCAL_DL'
    ag_name = 'ADVANCED_FOCAL_DL'
    ag_priority = 100
    _model_name = "AdvancedFocalDLModel"
    _model_type = "advanced_focal_dl_model"
    _typestr = "advanced_focal_dl_model_v1_advancedfocalloss"
    
    def _get_default_loss_function(self):
        """Advanced Focal Lossë¥¼ ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ë¡œ ë°˜í™˜"""
        # ë…¼ë¬¸ ë°©ì‹: Î±ì™€ base_Î³ëŠ” ì„¤ì •, Î³(t)ëŠ” ë‚´ë¶€ì—ì„œ ë™ì  ì¡°ì •
        alpha = getattr(self, 'focal_alpha', 0.25)
        base_gamma = getattr(self, 'base_gamma', 2.0)
        return AdvancedFocalLoss(alpha=alpha, base_gamma=base_gamma)
    
    def _set_params(self, **kwargs):
        """Advanced Focal Loss íŒŒë¼ë¯¸í„°ë¥¼ ì²˜ë¦¬"""
        print(f"ğŸ”§ AdvancedFocalDL _set_params í˜¸ì¶œë¨! kwargs={kwargs}")
        
        # ë…¼ë¬¸ ë°©ì‹: Î±ì™€ base_Î³ ì„¤ì • (Î³(t)ëŠ” ë‚´ë¶€ì—ì„œ ìë™ ì¡°ì •)
        self.focal_alpha = kwargs.pop('focal_alpha', 0.25)
        self.base_gamma = kwargs.pop('base_gamma', 2.0)
        
        print(f"ğŸ”§ AdvancedFocalDL _set_params: focal_alpha={self.focal_alpha}, base_gamma={self.base_gamma} (Î³(t)ëŠ” ìë™ ì¡°ì •)")
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ë¡œ ì „ë‹¬
        return super()._set_params(**kwargs)
    
    def _get_net(self, train_dataset, params):
        """Advanced Focal Loss íŒŒë¼ë¯¸í„°ë¥¼ ì²˜ë¦¬í•˜ê³  EmbedNetì— ì „ë‹¬"""
        print(f"ğŸ”§ AdvancedFocalDL _get_net í˜¸ì¶œë¨! params={params}")
        
        # ë…¼ë¬¸ ë°©ì‹: Î±ì™€ base_Î³ í•„í„°ë§ (Î³(t)ëŠ” ë‚´ë¶€ì—ì„œ ìë™ ì¡°ì •)
        filtered_params = params.copy()
        focal_alpha = filtered_params.pop('focal_alpha', 0.25)
        base_gamma = filtered_params.pop('base_gamma', 2.0)
        
        # selfì— íŒŒë¼ë¯¸í„° ì €ì¥
        self.focal_alpha = focal_alpha
        self.base_gamma = base_gamma
        
        print(f"ğŸ”§ AdvancedFocalDL _get_net: focal_alpha={focal_alpha}, base_gamma={base_gamma} (Î³(t)ëŠ” ìë™ ì¡°ì •)")
        print(f"ğŸ”§ AdvancedFocalDL ëª¨ë¸ êµ¬ì¡°: TabularNeuralNetTorchModel ìƒì†, AdaFocal Loss ì ìš©")
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ë¡œ EmbedNet ìƒì„±
        return super()._get_net(train_dataset, filtered_params)
    
    def _train_net(self, train_dataset, loss_kwargs, batch_size, num_epochs, 
                   epochs_wo_improve, val_dataset=None, test_dataset=None, 
                   time_limit=None, reporter=None, verbosity=2):
        """Advanced Focal Loss í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§"""
        print(f"ğŸš€ AdvancedFocalDL _train_net ì‹œì‘!")
        print(f"ğŸ”§ AdaFocal Loss íŒŒë¼ë¯¸í„°: alpha={self.focal_alpha} (Î³ëŠ” ìë™ ì¡°ì •)")
        print(f"ğŸ”§ ì†ì‹¤ í•¨ìˆ˜: {type(self._get_default_loss_function()).__name__}")
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ í•™ìŠµ ë©”ì„œë“œ í˜¸ì¶œ
        result = super()._train_net(train_dataset, loss_kwargs, batch_size, num_epochs,
                                  epochs_wo_improve, val_dataset, test_dataset,
                                  time_limit, reporter, verbosity)
        
        print(f"âœ… AdvancedFocalDL _train_net ì™„ë£Œ!")
        return result