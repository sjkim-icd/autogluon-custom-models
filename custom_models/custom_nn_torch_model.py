from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel

class CustomNNTorchModel(TabularNeuralNetTorchModel):
    ag_key = "CUSTOM_NN_TORCH"
    ag_name = "CUSTOM_NN_TORCH"
    ag_priority = 100
    _model_name = "CustomNNTorchModel"
    _model_type = "custom_nn_torch_model"
    _typestr = "custom_nn_torch_model_v1_crossentropy"
    
    def _set_params(self, **kwargs):
        """LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë¥¼ í•„í„°ë§"""
        print(f"ðŸ”§ CustomNNTorch _set_params í˜¸ì¶œë¨! kwargs={kwargs}")
        
        # LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ì„ popí•´ì„œ ì €ìž¥
        self.lr_scheduler = kwargs.pop('lr_scheduler', True)
        self.scheduler_type = kwargs.pop('scheduler_type', 'cosine')
        self.lr_scheduler_min_lr = kwargs.pop('lr_scheduler_min_lr', 1e-6)
        
        print(f"ðŸ”§ CustomNNTorch _set_params: lr_scheduler={self.lr_scheduler}, scheduler_type={self.scheduler_type}, min_lr={self.lr_scheduler_min_lr}")
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” ë¶€ëª¨ í´ëž˜ìŠ¤ë¡œ ì „ë‹¬
        return super()._set_params(**kwargs)
    
    def _get_net(self, train_dataset, params):
        """LR scheduler íŒŒë¼ë¯¸í„°ë¥¼ í•„í„°ë§í•´ì„œ EmbedNetì— ì „ë‹¬"""
        print(f"ðŸ”§ CustomNNTorch _get_net í˜¸ì¶œë¨! params={params}")
        
        # LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ì„ í•„í„°ë§
        filtered_params = params.copy()
        lr_scheduler = filtered_params.pop('lr_scheduler', True)
        scheduler_type = filtered_params.pop('scheduler_type', 'cosine')
        lr_scheduler_min_lr = filtered_params.pop('lr_scheduler_min_lr', 1e-6)
        
        # selfì— LR ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„° ì €ìž¥ (fallbackìš©)
        self.lr_scheduler = lr_scheduler
        self.scheduler_type = scheduler_type
        self.lr_scheduler_min_lr = lr_scheduler_min_lr
        
        print(f"ðŸ”§ CustomNNTorch _get_net: selfì— LR ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„° ì €ìž¥ë¨")
        print(f"ðŸ”§ CustomNNTorch _get_net: lr_scheduler={lr_scheduler}, scheduler_type={scheduler_type}, min_lr={lr_scheduler_min_lr}")
        
        # ë¶€ëª¨ í´ëž˜ìŠ¤ì˜ ê¸°ë³¸ _get_net í˜¸ì¶œ (í•„í„°ë§ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        model = super()._get_net(train_dataset, filtered_params)
        
        # ëª¨ë¸ì´ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ LR scheduler íŒŒë¼ë¯¸í„° ì„¤ì •
        if model is not None:
            model.lr_scheduler = lr_scheduler
            model.scheduler_type = scheduler_type
            model.lr_scheduler_min_lr = lr_scheduler_min_lr
            
            print(f"ðŸ”§ CustomNNTorch _get_net: modelì—ë„ LR ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„° ì„¤ì •ë¨")
        else:
            print(f"âš ï¸ CustomNNTorch _get_net: modelì´ Noneìž…ë‹ˆë‹¤. selfì—ë§Œ LR scheduler ì„¤ì •ë¨")
        
        return model
    
    def _train_net(
        self,
        train_dataset,
        loss_kwargs,
        batch_size,
        num_epochs,
        epochs_wo_improve,
        val_dataset=None,
        test_dataset=None,
        time_limit=None,
        reporter=None,
        verbosity=2,
    ):
        """DCNv2ì™€ ë™ì¼í•œ í˜•ì‹ì˜ ìƒì„¸í•œ epochë³„ ì¶œë ¥"""
        print("ðŸš€ CustomNNTorch _train_net í˜¸ì¶œë¨!")
        import torch
        import torch.optim.lr_scheduler as lr_scheduler
        import time
        import logging
        import io

        start_time = time.time()
        logging.debug("initializing neural network...")
        self.model.init_params()
        logging.debug("initialized")
        train_dataloader = train_dataset.build_loader(batch_size, self.num_dataloading_workers, is_test=False)

        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get("loss_function", "auto") == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()
        
        # LR scheduler ì„¤ì • (optimizer ìƒì„± í›„)
        scheduler = None
        print(f"ðŸ” CustomNNTorch Debug: self.lr_scheduler = {getattr(self, 'lr_scheduler', 'NOT_SET')}")
        print(f"ðŸ” CustomNNTorch Debug: self.scheduler_type = {getattr(self, 'scheduler_type', 'NOT_SET')}")
        print(f"ðŸ” CustomNNTorch Debug: hasattr(self, 'lr_scheduler') = {hasattr(self, 'lr_scheduler')}")
        print(f"ðŸ” CustomNNTorch Debug: self.lr_scheduler (if exists) = {self.lr_scheduler if hasattr(self, 'lr_scheduler') else 'N/A'}")
        
        # ìˆ˜ì •: _set_paramsì—ì„œ ì €ìž¥í•œ LR ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„° ì§ì ‘ ì‚¬ìš©
        if hasattr(self, 'lr_scheduler') and self.lr_scheduler:
            if hasattr(self, 'scheduler_type') and self.scheduler_type == 'cosine':
                scheduler = lr_scheduler.CosineAnnealingLR(
                    self.optimizer,
                    T_max=num_epochs,
                    eta_min=self.lr_scheduler_min_lr
                )
                print(f"âœ… CustomNNTorch: Cosine Annealing LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨ (min_lr={self.lr_scheduler_min_lr})")
            else:
                scheduler = lr_scheduler.ReduceLROnPlateau(
                    self.optimizer,
                    mode='max',
                    factor=0.2,
                    patience=5,
                    min_lr=self.lr_scheduler_min_lr
                )
                print(f"âœ… CustomNNTorch: ReduceLROnPlateau LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨")
        else:
            print(f"âŒ CustomNNTorch: LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        # DCNv2ì™€ ë™ì¼í•œ í˜•ì‹ì˜ ìƒì„¸í•œ í•™ìŠµ ë£¨í”„ í˜¸ì¶œ
        self._train_with_scheduler(train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity, scheduler)
    
    def _train_with_scheduler(
        self,
        train_dataset,
        loss_kwargs,
        batch_size,
        num_epochs,
        epochs_wo_improve,
        val_dataset=None,
        test_dataset=None,
        time_limit=None,
        reporter=None,
        verbosity=2,
        scheduler=None,
    ):
        """ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í•¨ê»˜ í•™ìŠµí•˜ë©´ì„œ LR ëª¨ë‹ˆí„°ë§"""
        import torch
        import time
        import io

        # ê¸°ë³¸ ì„¤ì •
        self.model.init_params()
        train_dataloader = train_dataset.build_loader(
            batch_size, self.num_dataloading_workers, is_test=False
        )

        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get(
            "loss_function", "auto"
        ) == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()

        # Early stopping ì„¤ì •
        if epochs_wo_improve is not None:
            early_stopping_method = self._get_early_stopping_strategy(
                num_rows_train=len(train_dataset)
            )
        else:
            early_stopping_method = self._get_early_stopping_strategy(
                num_rows_train=len(train_dataset)
            )

        # Validation ë¼ë²¨
        if val_dataset is not None:
            y_val = val_dataset.get_labels()
            if y_val.ndim == 2 and y_val.shape[1] == 1:
                y_val = y_val.flatten()
        else:
            y_val = None

        # ëª¨ë¸ ì €ìž¥ìš© ë²„í¼
        io_buffer = None
        best_epoch = 0
        best_val_metric = -float("inf")

        # í•™ìŠµ ë£¨í”„
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            num_batches = 0

            # í˜„ìž¬ LR ì¶œë ¥
            current_lr = self.optimizer.param_groups[0]["lr"]

            # ë°°ì¹˜ë³„ í•™ìŠµ
            for batch_idx, data_batch in enumerate(train_dataloader):
                self.optimizer.zero_grad()
                loss = self.model.compute_loss(data_batch, **loss_kwargs)
                loss.backward()

                # Gradient clipping (NaN ë°©ì§€)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                total_loss += loss.item()
                num_batches += 1

            # ì—í¬í¬ í‰ê·  ì†ì‹¤
            avg_loss = total_loss / num_batches
            epoch_time = time.time() - epoch_start_time

            # ---------------------------
            # Validation í‰ê°€ (f1 + loss)
            # ---------------------------
            val_metric = None
            val_loss = None
            if val_dataset is not None:
                # f1 (AutoGluon eval_metric ê¸°ë°˜)
                val_metric = self.score(
                    X=val_dataset, y=y_val, metric=self.stopping_metric, _reset_threads=False
                )

                # validation loss (scheduler ìš©)
                with torch.no_grad():
                    val_loss_total = 0.0
                    val_batches = 0
                    for data_batch in val_dataset.build_loader(
                        batch_size, self.num_dataloading_workers, is_test=True
                    ):
                        loss = self.model.compute_loss(data_batch, **loss_kwargs)
                        val_loss_total += loss.item()
                        val_batches += 1
                    val_loss = val_loss_total / max(1, val_batches)

                # Best model ì €ìž¥ (f1 ê¸°ì¤€)
                if val_metric > best_val_metric:
                    best_val_metric = val_metric
                    io_buffer = io.BytesIO()
                    torch.save(self.model, io_buffer)
                    best_epoch = epoch + 1

            # ---------------------------
            # ë¡œê·¸ ì¶œë ¥
            # ---------------------------
            import time as time_module

            current_time = time_module.strftime("%Y-%m-%d %H:%M:%S")
            if val_metric is not None:
                log_msg = (
                    f"[{current_time}] Epoch {epoch+1}/{num_epochs}: "
                    f"Train loss: {avg_loss:.4f}, "
                    f"Val {self.stopping_metric.name}: {val_metric:.4f}, "
                    f"Val loss: {val_loss:.4f}, "
                    f"LR: {current_lr:.2e}, "
                    f"Best Epoch: {best_epoch}, "
                    f"Time: {epoch_time:.2f}s"
                )
            else:
                log_msg = (
                    f"[{current_time}] Epoch {epoch+1}/{num_epochs}: "
                    f"Train loss: {avg_loss:.4f}, "
                    f"LR: {current_lr:.2e}, "
                    f"Time: {epoch_time:.2f}s"
                )
            print(log_msg)

            # ---------------------------
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            # ---------------------------
            if scheduler:
                if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingLR):
                    scheduler.step()
                elif isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    # âœ… Validation loss ê¸°ì¤€ (í‘œì¤€ì ì¸ ì ‘ê·¼)
                    scheduler.step(val_loss if val_loss is not None else avg_loss)

            # ---------------------------
            # Early stopping ì²´í¬ (f1 ê¸°ì¤€)
            # ---------------------------
            if val_dataset is not None:
                is_best = val_metric > best_val_metric
                early_stop = early_stopping_method.update(
                    cur_round=epoch, is_best=is_best
                )
                if early_stop:
                    print(f"   Early stopping at epoch {epoch+1}")
                    break

        # ---------------------------
        # Best model ë¡œë“œ
        # ---------------------------
        if io_buffer is not None:
            io_buffer.seek(0)
            self.model = torch.load(io_buffer, weights_only=False)
            print(
                f"   Best model loaded from epoch {best_epoch} "
                f"(Val {self.stopping_metric.name}: {best_val_metric:.4f})"
            )

        # save trained parameters
        self.params_trained["batch_size"] = batch_size
        self.params_trained["num_epochs"] = best_epoch 