from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel
import torch.nn as nn
import torch
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Focal Loss (ë…¼ë¬¸ ì›ë³¸ ìˆ˜ì‹ êµ¬í˜„)
    
    ë…¼ë¬¸ ìˆ˜ì‹:
        FL(p_t) = -Î±_t * (1 - p_t)^Î³ * log(p_t)

    ì—¬ê¸°ì„œ,
        - p_t: ì •ë‹µ í´ëž˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
        - Î±_t: í´ëž˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜
          * Î±_t = Î± if y = 1 (positive class)
          * Î±_t = 1 - Î± if y = 0 (negative class)
        - Î³: focusing parameter (ë…¼ë¬¸ì—ì„œëŠ” Î³ = 2)
        - (1 - p_t)^Î³: ì‰¬ìš´ ìƒ˜í”Œì˜ ì†ì‹¤ì„ ì¤„ì´ëŠ” modulating factor

    ë…¼ë¬¸ ê¶Œìž¥ê°’:
        - Î± = 0.25 (positive class ê°€ì¤‘ì¹˜)
        - Î³ = 2.0 (focusing parameter)

    ì°¸ê³ :
        CrossEntropyLossëŠ” -log(p_t)ë§Œ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´,
        FocalLossëŠ” ê±°ê¸°ì— (1 - p_t)^Î³ë¥¼ ê³±í•´ì„œ 'ì‰¬ìš´ ìƒ˜í”Œ'ì˜ ì†ì‹¤ì„ ì¤„ì—¬ì¤Œ
        Î±_të¥¼ í†µí•´ í´ëž˜ìŠ¤ ë¶ˆê· í˜•ë„ í•¨ê»˜ ì²˜ë¦¬
    """

    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        """
        inputs: (batch_size, num_classes) - raw logits
        targets: (batch_size,) - class indices (e.g., 0 ë˜ëŠ” 1)
        """

        # 1. log(p_c): CrossEntropyì²˜ëŸ¼ softmax + log ë¥¼ í•œ ë²ˆì— ìˆ˜í–‰
        log_probs = F.log_softmax(inputs, dim=1)  # shape: (B, C)

        # 2. p_c: log(p_c)ë¥¼ ì§€ìˆ˜í™”í•˜ì—¬ í™•ë¥ ë¡œ ë³µì›
        probs = torch.exp(log_probs)              # shape: (B, C)

        # 3. ì •ë‹µ í´ëž˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ longìœ¼ë¡œ ë³€í™˜
        targets = targets.long()

        # 4. p_cì—ì„œ ì •ë‹µ í´ëž˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í™•ë¥  p_t ì¶”ì¶œ
        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)      # shape: (B,)
        log_pt = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)  # log(p_t)

        # 5. ë…¼ë¬¸ ìˆ˜ì‹: Î±_t ê³„ì‚°
        # Î±_t = Î± if y = 1 (positive class), Î±_t = 1 - Î± if y = 0 (negative class)
        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)

        # 6. focal loss ê³µì‹ ì ìš©: -Î±_t * (1 - p_t)^Î³ * log(p_t)
        loss = -alpha_t * (1 - pt) ** self.gamma * log_pt

        # 7. í‰ê·  or í•©ì‚°
        if self.reduction == 'mean':
            return loss.mean()
        else:
            return loss.sum()


class CustomFocalDLModel(TabularNeuralNetTorchModel):
    ag_key = "CUSTOM_FOCAL_DL"
    ag_name = "CUSTOM_FOCAL_DL"
    ag_priority = 100
    _model_name = "CustomFocalDLModel"
    _model_type = "custom_focal_dl_model"
    _typestr = "custom_focal_dl_model_v1_focalloss"
    
    def _get_default_loss_function(self):
        # ë…¼ë¬¸ ê¶Œìž¥ê°’ ì‚¬ìš©: Î±=0.25, Î³=2.0
        alpha = getattr(self, 'focal_alpha', 0.25)  # ê¸°ë³¸ê°’ì„ ë…¼ë¬¸ ê¶Œìž¥ê°’ìœ¼ë¡œ ë³€ê²½
        gamma = getattr(self, 'focal_gamma', 2.0)   # ê¸°ë³¸ê°’ì„ ë…¼ë¬¸ ê¶Œìž¥ê°’ìœ¼ë¡œ ë³€ê²½
        return FocalLoss(alpha=alpha, gamma=gamma)
    
    def _set_params(self, **kwargs):
        """Focal Loss íŒŒë¼ë¯¸í„°ì™€ LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë¥¼ ì²˜ë¦¬"""
        print(f"ðŸ”§ CustomFocalDL _set_params í˜¸ì¶œë¨! kwargs={kwargs}")
        
        # Focal Loss íŠ¹í™” íŒŒë¼ë¯¸í„° ì²˜ë¦¬ (ë…¼ë¬¸ ê¶Œìž¥ê°’ìœ¼ë¡œ ê¸°ë³¸ê°’ ë³€ê²½)
        self.focal_alpha = kwargs.pop('focal_alpha', 0.25)  # ë…¼ë¬¸ ê¶Œìž¥ê°’
        self.focal_gamma = kwargs.pop('focal_gamma', 2.0)   # ë…¼ë¬¸ ê¶Œìž¥ê°’
        
        # LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ì„ popí•´ì„œ ì €ìž¥
        self.lr_scheduler = kwargs.pop('lr_scheduler', True)
        self.scheduler_type = kwargs.pop('scheduler_type', 'cosine')
        self.lr_scheduler_min_lr = kwargs.pop('lr_scheduler_min_lr', 1e-6)
        
        print(f"ðŸ”§ CustomFocalDL _set_params: focal_alpha={self.focal_alpha}, focal_gamma={self.focal_gamma}")
        print(f"ðŸ”§ CustomFocalDL _set_params: lr_scheduler={self.lr_scheduler}, scheduler_type={self.scheduler_type}, min_lr={self.lr_scheduler_min_lr}")
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” ë¶€ëª¨ í´ëž˜ìŠ¤ë¡œ ì „ë‹¬
        return super()._set_params(**kwargs)
    
    def _get_net(self, train_dataset, params):
        """Focal Loss íŒŒë¼ë¯¸í„°ë¥¼ ì²˜ë¦¬í•˜ê³  EmbedNetì— ì „ë‹¬"""
        print(f"ðŸ”§ CustomFocalDL _get_net í˜¸ì¶œë¨! params={params}")
        
        # Focal Loss íŒŒë¼ë¯¸í„°ë“¤ì„ í•„í„°ë§
        filtered_params = params.copy()
        focal_alpha = filtered_params.pop('focal_alpha', 0.25)  # ë…¼ë¬¸ ê¶Œìž¥ê°’
        focal_gamma = filtered_params.pop('focal_gamma', 2.0)   # ë…¼ë¬¸ ê¶Œìž¥ê°’
        
        # LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ì„ í•„í„°ë§
        lr_scheduler = filtered_params.pop('lr_scheduler', True)
        scheduler_type = filtered_params.pop('scheduler_type', 'cosine')
        lr_scheduler_min_lr = filtered_params.pop('lr_scheduler_min_lr', 1e-6)
        
        # selfì— íŒŒë¼ë¯¸í„° ì €ìž¥
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        self.lr_scheduler = lr_scheduler
        self.scheduler_type = scheduler_type
        self.lr_scheduler_min_lr = lr_scheduler_min_lr
        
        print(f"ðŸ”§ CustomFocalDL _get_net: focal_alpha={focal_alpha}, focal_gamma={focal_gamma}")
        print(f"ðŸ”§ CustomFocalDL _get_net: lr_scheduler={lr_scheduler}, scheduler_type={scheduler_type}, min_lr={lr_scheduler_min_lr}")
        
        # ëžŒë‹¤ í•¨ìˆ˜ë¡œ ë©”ì„œë“œë¥¼ ë®ì–´ì“°ì§€ ì•Šê³ , ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ìž¥
        # self._get_default_loss_function = lambda: FocalLoss(alpha=focal_alpha, gamma=focal_gamma)
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ë¡œ EmbedNet ìƒì„±
        return super()._get_net(train_dataset, filtered_params)
    
    def _train_net(
        self,
        train_dataset,
        loss_kwargs,
        batch_size,
        num_epochs,
        epochs_wo_improve,
        val_dataset=None,
        test_dataset=None,
        time_limit=None,
        reporter=None,
        verbosity=2,
    ):
        """AutoGluonì˜ _train_net ë©”ì„œë“œë¥¼ ë³µì‚¬í•˜ê³  LR scheduler ì¶”ê°€"""
        print("ðŸš€ CustomFocalDL _train_net í˜¸ì¶œë¨!")  # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€
        import torch
        import torch.optim.lr_scheduler as lr_scheduler
        import time
        import logging
        import io
        from copy import deepcopy

        start_time = time.time()
        logging.debug("initializing neural network...")
        self.model.init_params()
        logging.debug("initialized")
        train_dataloader = train_dataset.build_loader(batch_size, self.num_dataloading_workers, is_test=False)

        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get("loss_function", "auto") == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()
        
        # LR scheduler ì„¤ì • (optimizer ìƒì„± í›„)
        scheduler = None
        print(f"ðŸ” CustomFocalDL Debug: self.lr_scheduler = {getattr(self, 'lr_scheduler', 'NOT_SET')}")
        print(f"ðŸ” CustomFocalDL Debug: self.scheduler_type = {getattr(self, 'scheduler_type', 'NOT_SET')}")
        print(f"ðŸ” CustomFocalDL Debug: hasattr(self, 'lr_scheduler') = {hasattr(self, 'lr_scheduler')}")
        print(f"ðŸ” CustomFocalDL Debug: self.lr_scheduler (if exists) = {self.lr_scheduler if hasattr(self, 'lr_scheduler') else 'N/A'}")
        
        # ìˆ˜ì •: _set_paramsì—ì„œ ì €ìž¥í•œ LR ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„° ì§ì ‘ ì‚¬ìš©
        if hasattr(self, 'lr_scheduler') and self.lr_scheduler:
            if hasattr(self, 'scheduler_type') and self.scheduler_type == 'cosine':
                scheduler = lr_scheduler.CosineAnnealingLR(
                    self.optimizer,
                    T_max=num_epochs,
                    eta_min=self.lr_scheduler_min_lr
                )
                print(f"âœ… CustomFocalDL: Cosine Annealing LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨ (min_lr={self.lr_scheduler_min_lr})")
            else:
                scheduler = lr_scheduler.ReduceLROnPlateau(
                    self.optimizer,
                    mode='max',
                    factor=0.2,
                    patience=5,
                    min_lr=self.lr_scheduler_min_lr
                )
                print(f"âœ… CustomFocalDL: ReduceLROnPlateau LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨")
        else:
            print(f"âŒ CustomFocalDL: LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        # DCNv2ì™€ ë™ì¼í•œ í˜•ì‹ì˜ ìƒì„¸í•œ í•™ìŠµ ë£¨í”„ í˜¸ì¶œ
        self._train_with_scheduler(train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity, scheduler)
    
    def _train_with_scheduler(
        self,
        train_dataset,
        loss_kwargs,
        batch_size,
        num_epochs,
        epochs_wo_improve,
        val_dataset=None,
        test_dataset=None,
        time_limit=None,
        reporter=None,
        verbosity=2,
        scheduler=None,
    ):
        """ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í•¨ê»˜ í•™ìŠµí•˜ë©´ì„œ LR ëª¨ë‹ˆí„°ë§"""
        import torch
        import time
        import io

        # ê¸°ë³¸ ì„¤ì •
        self.model.init_params()
        train_dataloader = train_dataset.build_loader(
            batch_size, self.num_dataloading_workers, is_test=False
        )

        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get(
            "loss_function", "auto"
        ) == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()

        # Early stopping ì„¤ì •
        if epochs_wo_improve is not None:
            early_stopping_method = self._get_early_stopping_strategy(
                num_rows_train=len(train_dataset)
            )
        else:
            early_stopping_method = self._get_early_stopping_strategy(
                num_rows_train=len(train_dataset)
            )

        # Validation ë¼ë²¨
        if val_dataset is not None:
            y_val = val_dataset.get_labels()
            if y_val.ndim == 2 and y_val.shape[1] == 1:
                y_val = y_val.flatten()
        else:
            y_val = None

        # ëª¨ë¸ ì €ìž¥ìš© ë²„í¼
        io_buffer = None
        best_epoch = 0
        best_val_metric = -float("inf")

        # í•™ìŠµ ë£¨í”„
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            num_batches = 0

            # í˜„ìž¬ LR ì¶œë ¥
            current_lr = self.optimizer.param_groups[0]["lr"]

            # ë°°ì¹˜ë³„ í•™ìŠµ
            for batch_idx, data_batch in enumerate(train_dataloader):
                self.optimizer.zero_grad()
                loss = self.model.compute_loss(data_batch, **loss_kwargs)
                loss.backward()

                # Gradient clipping (NaN ë°©ì§€)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                total_loss += loss.item()
                num_batches += 1

            # ì—í¬í¬ í‰ê·  ì†ì‹¤
            avg_loss = total_loss / num_batches
            epoch_time = time.time() - epoch_start_time

            # ---------------------------
            # Validation í‰ê°€ (f1 + loss)
            # ---------------------------
            val_metric = None
            val_loss = None
            if val_dataset is not None:
                # f1 (AutoGluon eval_metric ê¸°ë°˜)
                val_metric = self.score(
                    X=val_dataset, y=y_val, metric=self.stopping_metric, _reset_threads=False
                )

                # validation loss (scheduler ìš©)
                with torch.no_grad():
                    val_loss_total = 0.0
                    val_batches = 0
                    for data_batch in val_dataset.build_loader(
                        batch_size, self.num_dataloading_workers, is_test=True
                    ):
                        loss = self.model.compute_loss(data_batch, **loss_kwargs)
                        val_loss_total += loss.item()
                        val_batches += 1
                    val_loss = val_loss_total / max(1, val_batches)

                # Best model ì €ìž¥ (f1 ê¸°ì¤€)
                if val_metric > best_val_metric:
                    best_val_metric = val_metric
                    io_buffer = io.BytesIO()
                    torch.save(self.model, io_buffer)
                    best_epoch = epoch + 1

            # ---------------------------
            # ë¡œê·¸ ì¶œë ¥
            # ---------------------------
            import time as time_module

            current_time = time_module.strftime("%Y-%m-%d %H:%M:%S")
            if val_metric is not None:
                log_msg = (
                    f"[{current_time}] Epoch {epoch+1}/{num_epochs}: "
                    f"Train loss: {avg_loss:.4f}, "
                    f"Val {self.stopping_metric.name}: {val_metric:.4f}, "
                    f"Val loss: {val_loss:.4f}, "
                    f"LR: {current_lr:.2e}, "
                    f"Best Epoch: {best_epoch}, "
                    f"Time: {epoch_time:.2f}s"
                )
            else:
                log_msg = (
                    f"[{current_time}] Epoch {epoch+1}/{num_epochs}: "
                    f"Train loss: {avg_loss:.4f}, "
                    f"LR: {current_lr:.2e}, "
                    f"Time: {epoch_time:.2f}s"
                )
            print(log_msg)

            # ---------------------------
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            # ---------------------------
            if scheduler:
                if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingLR):
                    scheduler.step()
                elif isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    # âœ… Validation loss ê¸°ì¤€ (í‘œì¤€ì ì¸ ì ‘ê·¼)
                    scheduler.step(val_loss if val_loss is not None else avg_loss)

            # ---------------------------
            # Early stopping ì²´í¬ (f1 ê¸°ì¤€)
            # ---------------------------
            if val_dataset is not None:
                is_best = val_metric > best_val_metric
                early_stop = early_stopping_method.update(
                    cur_round=epoch, is_best=is_best
                )
                if early_stop:
                    print(f"   Early stopping at epoch {epoch+1}")
                    break

        # ---------------------------
        # Best model ë¡œë“œ
        # ---------------------------
        if io_buffer is not None:
            io_buffer.seek(0)
            self.model = torch.load(io_buffer, weights_only=False)
            print(
                f"   Best model loaded from epoch {best_epoch} "
                f"(Val {self.stopping_metric.name}: {best_val_metric:.4f})"
            )

        # save trained parameters
        self.params_trained["batch_size"] = batch_size
        self.params_trained["num_epochs"] = best_epoch 