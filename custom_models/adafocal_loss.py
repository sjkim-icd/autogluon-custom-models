"""
AdaFocal: Calibration-aware Adaptive Focal Loss

This implementation follows the original AdaFocal paper:
"AdaFocal: Calibration-aware Adaptive Focal Loss" (NeurIPS 2022)
Authors: Arindam Ghosh, Thomas Schaaf, Matt Gormley

Key Ideas:
1. Validation set ê¸°ë°˜ calibration ì¸¡ì •
2. Sampleë³„ ê°œë³„ Î³ ì¡°ì •
3. Calibration feedbackì„ í†µí•œ adaptive learning
4. ì´ì „ Î³ ê°’ ê¸°ì–µ (momentum)

Mathematical Foundation:
- FL(p_t) = -Î± * (1 - p_t)^Î³(t) * log(p_t)
- Î³(t) = f(Î³(t-1), calibration_feedback)
- Calibration error = |ì •í™•ë„ - ì˜ˆì¸¡í™•ë¥ |
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, Dict, List
from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel


class AdaFocalLoss(nn.Module):
    """
    AdaFocal Loss: Calibration-aware Adaptive Focal Loss
    
    This implementation follows the AdaFocal paper exactly:
    - Î³ is updated per confidence bin (not per sample)
    - Updates happen every batch based on validation set calibration
    - Switches between focal loss (Î³ > 0) and inverse-focal loss (Î³ < 0)
    
    Reference:
    - Paper: https://proceedings.neurips.cc/paper_files/paper/2022/file/0a692a24dbc744fca340b9ba33bc6522-Paper-Conference.pdf
    - Code: https://github.com/3mcloud/adafocal
    
    í•œê¸€ ì„¤ëª…:
    AdaFocal LossëŠ” ì‹ ë¢°ë„ ë³´ì •ì„ ì¸ì‹í•˜ëŠ” ì ì‘í˜• focal lossì…ë‹ˆë‹¤.
    - Î³ëŠ” ê° ì‹ ë¢°ë„ êµ¬ê°„ë³„ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤ (ìƒ˜í”Œë³„ì´ ì•„ë‹˜)
    - ë§¤ ë°°ì¹˜ë§ˆë‹¤ ê²€ì¦ ì„¸íŠ¸ì˜ ë³´ì • ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤
    - ê³¼ì‹ ì ì¼ ë•ŒëŠ” focal loss (Î³ > 0), ê³¼ì†Œì‹ ì ì¼ ë•ŒëŠ” inverse-focal loss (Î³ < 0) ì‚¬ìš©
    """
    
    def __init__(self,
                 num_classes: int = 2,
                 alpha: float = 1.0,
                 base_gamma: float = 2.0,
                 momentum: float = 0.9,
                 num_bins: int = 15,  # Paper uses 15 equal-mass bins (ë…¼ë¬¸ì—ì„œëŠ” 15ê°œ ë™ì¼ ì§ˆëŸ‰ êµ¬ê°„ ì‚¬ìš©)
                 calibration_threshold: float = 0.2,  # Sth in paper (ë…¼ë¬¸ì˜ Sth ê°’)
                 lambda_param: float = 1.0,  # Î» in paper (ë…¼ë¬¸ì˜ Î» íŒŒë¼ë¯¸í„°) - ì´ ë¶€ë¶„ ì¶”ê°€!
                 gamma_max: float = 20.0,  # Î³max in paper (ë…¼ë¬¸ì˜ Î³max ê°’)
                 gamma_min: float = -2.0):  # Î³min in paper (ë…¼ë¬¸ì˜ Î³min ê°’)
        super(AdaFocalLoss, self).__init__()
        
        self.num_classes = num_classes
        self.alpha = alpha
        self.base_gamma = base_gamma
        self.momentum = momentum
        self.num_bins = num_bins
        self.calibration_threshold = calibration_threshold
        self.lambda_param = lambda_param  # ì´ ë¶€ë¶„ ì¶”ê°€!
        self.gamma_max = gamma_max
        self.gamma_min = gamma_min
        
        # Initialize Î³ for each bin (paper: Î³t=0,i = 1 for all bins)
        # ê° êµ¬ê°„ë³„ë¡œ Î³ ì´ˆê¸°í™” (ë…¼ë¬¸: ëª¨ë“  êµ¬ê°„ì—ì„œ Î³t=0,i = 1)
        self.register_buffer('gamma_bins', torch.ones(num_bins))
        
        # Validation cache for calibration measurement
        # ë³´ì • ì¸¡ì •ì„ ìœ„í•œ ê²€ì¦ ì„¸íŠ¸ ìºì‹œ
        self.validation_cache = {
            'predictions': [],
            'targets': [],
            'confidences': []
        }
        
        # Bin edges for equal-mass binning
        # ë™ì¼ ì§ˆëŸ‰ êµ¬ê°„ì„ ìœ„í•œ êµ¬ê°„ ê²½ê³„
        self.bin_edges = torch.linspace(0, 1, num_bins + 1)
        
        # Training step counter
        # í›ˆë ¨ ë‹¨ê³„ ì¹´ìš´í„°
        self.training_step = 0
        
        # Sampleë³„ ê°œë³„ Î³ ì €ì¥
        self.register_buffer('gamma_per_sample', torch.ones(1000) * base_gamma)  # ì„ì‹œ í¬ê¸°
        self.register_buffer('gamma_momentum', torch.ones(1000) * base_gamma)
        
        # Calibration tracking
        self.register_buffer('calibration_history', torch.zeros(num_bins))
        self.register_buffer('sample_count_history', torch.zeros(num_bins))
        
        # Sample difficulty tracking
        self.register_buffer('sample_difficulties', torch.zeros(1000))
        
        print(f"ğŸ”§ AdaFocal Loss ì´ˆê¸°í™”:")
        print(f"   ğŸ“Š Base Î³: {base_gamma}")
        print(f"   ğŸ“Š Momentum: {momentum}")
        print(f"   ğŸ“Š Calibration bins: {num_bins}")
        print(f"   ğŸ“Š Calibration threshold: {calibration_threshold}")
    
    def update_validation_cache(self, predictions: torch.Tensor, targets: torch.Tensor):
        """
        Validation setì—ì„œ calibration ì •ë³´ ì—…ë°ì´íŠ¸
        
        Args:
            predictions: ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥  (N, num_classes)
            targets: ì‹¤ì œ ë ˆì´ë¸” (N,)
        """
        with torch.no_grad():
            # ì˜ˆì¸¡ í™•ì‹ ë„ (ì •ë‹µ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ )
            confidences = predictions.gather(1, targets.unsqueeze(1)).squeeze(1)
            
            # Validation cacheì— ì €ì¥
            self.validation_cache['predictions'].append(predictions.detach().cpu())
            self.validation_cache['targets'].append(targets.detach().cpu())
            self.validation_cache['confidences'].append(confidences.detach().cpu())
            
            print(f"ğŸ”„ Validation cache ì—…ë°ì´íŠ¸: {len(self.validation_cache['predictions'])} batches")
    
    def measure_calibration(self) -> Dict[str, float]:
        """
        Validation setì—ì„œ calibration ì¸¡ì •
        
        Returns:
            calibration_metrics: calibration ê´€ë ¨ ì§€í‘œë“¤
        """
        if not self.validation_cache['predictions']:
            print("âš ï¸ Validation cacheê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¨¼ì € update_validation_cacheë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
            return {}
        
        # ëª¨ë“  validation ë°ì´í„° í•©ì¹˜ê¸°
        all_predictions = torch.cat(self.validation_cache['predictions'], dim=0)
        all_targets = torch.cat(self.validation_cache['targets'], dim=0)
        all_confidences = torch.cat(self.validation_cache['confidences'], dim=0)
        
        # Calibration binsë³„ë¡œ ì •í™•ë„ ê³„ì‚°
        bin_accuracies = torch.zeros(self.num_bins)
        bin_counts = torch.zeros(self.num_bins)
        
        for i in range(self.num_bins):
            # ië²ˆì§¸ binì— ì†í•˜ëŠ” ìƒ˜í”Œë“¤ ì°¾ê¸°
            bin_mask = (all_confidences >= self.bin_edges[i]) & (all_confidences < self.bin_edges[i + 1])
            
            if bin_mask.sum() > 0:
                # í•´ë‹¹ binì˜ ì •í™•ë„ ê³„ì‚°
                bin_predictions = all_predictions[bin_mask]
                bin_targets = all_targets[bin_mask]
                
                # ì˜ˆì¸¡ í´ë˜ìŠ¤
                pred_classes = torch.argmax(bin_predictions, dim=1)
                
                # ì •í™•ë„
                accuracy = (pred_classes == bin_targets).float().mean()
                bin_accuracies[i] = accuracy
                bin_counts[i] = bin_mask.sum()
        
        # Expected Calibration Error (ECE) ê³„ì‚°
        ece = 0.0
        for i in range(self.num_bins):
            if bin_counts[i] > 0:
                ece += abs(bin_accuracies[i] - self.bin_centers[i]) * bin_counts[i]
        ece = ece / all_confidences.shape[0]
        
        # Calibration metrics
        calibration_metrics = {
            'ece': ece.item(),
            'bin_accuracies': bin_accuracies.cpu().numpy(),
            'bin_counts': bin_counts.cpu().numpy(),
            'bin_centers': self.bin_centers.cpu().numpy()
        }
        
        print(f"ğŸ“Š Calibration ì¸¡ì • ê²°ê³¼:")
        print(f"   ğŸ“Š ECE: {ece:.4f}")
        print(f"   ğŸ“Š Binë³„ ì •í™•ë„: {bin_accuracies.cpu().numpy()}")
        print(f"   ğŸ“Š Binë³„ ìƒ˜í”Œ ìˆ˜: {bin_counts.cpu().numpy()}")
        
        return calibration_metrics
    
    def adjust_gamma_based_on_calibration(self, sample_indices: torch.Tensor):
        """
        Calibration ê²°ê³¼ì— ë”°ë¼ sampleë³„ Î³ ì¡°ì •
        
        Args:
            sample_indices: í˜„ì¬ ë°°ì¹˜ì˜ ìƒ˜í”Œ ì¸ë±ìŠ¤
        """
        if not self.validation_cache['predictions']:
            return
        
        # Calibration ì¸¡ì •
        calibration_metrics = self.measure_calibration()
        
        if not calibration_metrics:
            return
        
        # í˜„ì¬ ë°°ì¹˜ì˜ ì˜ˆì¸¡ í™•ë¥ ê³¼ íƒ€ê²Ÿ
        current_batch_size = sample_indices.shape[0]
        
        # Sampleë³„ Î³ ì¡°ì •
        for i, sample_idx in enumerate(sample_indices):
            if sample_idx >= self.gamma_per_sample.shape[0]:
                # ë²„í¼ í¬ê¸° í™•ì¥
                new_size = max(sample_idx + 1, self.gamma_per_sample.shape[0] * 2)
                self.gamma_per_sample = torch.cat([
                    self.gamma_per_sample,
                    torch.ones(new_size - self.gamma_per_sample.shape[0]) * self.base_gamma
                ]).to(self.gamma_per_sample.device)
                self.gamma_momentum = torch.cat([
                    self.gamma_momentum,
                    torch.ones(new_size - self.gamma_momentum.shape[0]) * self.base_gamma
                ]).to(self.gamma_momentum.device)
            
            # í˜„ì¬ ìƒ˜í”Œì˜ Î³ ê°’
            current_gamma = self.gamma_per_sample[sample_idx]
            
            # Calibration ê¸°ë°˜ Î³ ì¡°ì •
            if calibration_metrics['ece'] > self.calibration_threshold:
                # ECEê°€ ë†’ìœ¼ë©´ Î³ë¥¼ ë” ì ê·¹ì ìœ¼ë¡œ ì¡°ì •
                if i < len(calibration_metrics['bin_accuracies']):
                    bin_idx = min(i, len(calibration_metrics['bin_accuracies']) - 1)
                    bin_accuracy = calibration_metrics['bin_accuracies'][bin_idx]
                    bin_center = calibration_metrics['bin_centers'][bin_idx]
                    
                    # Calibration error
                    cal_error = abs(bin_accuracy - bin_center)
                    
                    if cal_error > 0.1:  # Calibration errorê°€ í° ê²½ìš°
                        if bin_accuracy < bin_center:  # Under-confident
                            new_gamma = current_gamma * 1.1  # Î³ ì¦ê°€
                        else:  # Over-confident
                            new_gamma = current_gamma * 0.9  # Î³ ê°ì†Œ
                        
                        # Momentum ê¸°ë°˜ ì—…ë°ì´íŠ¸
                        self.gamma_per_sample[sample_idx] = (
                            self.momentum * current_gamma + 
                            (1 - self.momentum) * new_gamma
                        )
        
        print(f"ğŸ”„ Sampleë³„ Î³ ì¡°ì • ì™„ë£Œ (ë°°ì¹˜ í¬ê¸°: {current_batch_size})")
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor, 
                sample_indices: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        AdaFocal Loss ê³„ì‚°
        
        Args:
            inputs: ëª¨ë¸ ì¶œë ¥ (N, num_classes)
            targets: ì‹¤ì œ ë ˆì´ë¸” (N,)
            sample_indices: ìƒ˜í”Œ ì¸ë±ìŠ¤ (N,) - calibration trackingìš©
        
        Returns:
            loss: AdaFocal loss ê°’
        """
        batch_size = inputs.shape[0]
        
        # Sample indicesê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        if sample_indices is None:
            sample_indices = torch.arange(batch_size, device=inputs.device)
        
        # Î³ ì¡°ì • (calibration ê¸°ë°˜)
        self.adjust_gamma_based_on_calibration(sample_indices)
        
        # Sampleë³„ Î³ ê°’ ê°€ì ¸ì˜¤ê¸°
        gamma_values = torch.zeros(batch_size, device=inputs.device)
        for i, sample_idx in enumerate(sample_indices):
            if sample_idx < self.gamma_per_sample.shape[0]:
                gamma_values[i] = self.gamma_per_sample[sample_idx]
            else:
                gamma_values[i] = self.base_gamma
        
        # Cross entropy loss ê³„ì‚°
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # ì˜ˆì¸¡ í™•ë¥ 
        probs = F.softmax(inputs, dim=1)
        target_probs = probs.gather(1, targets.unsqueeze(1)).squeeze(1)
        
        # Focal weights: (1 - p_t)^Î³
        focal_weights = (1 - target_probs) ** gamma_values
        
        # ìµœì¢… loss: -Î± * (1 - p_t)^Î³ * log(p_t)
        loss = (self.alpha * focal_weights * ce_loss).mean()
        
        # Sample difficulty tracking
        with torch.no_grad():
            difficulties = 1.0 - target_probs
            for i, sample_idx in enumerate(sample_indices):
                if sample_idx < self.sample_difficulties.shape[0]:
                    self.sample_difficulties[sample_idx] = difficulties[i]
        
        return loss
    
    def get_calibration_summary(self) -> Dict[str, any]:
        """
        Calibration ìƒíƒœ ìš”ì•½ ë°˜í™˜
        """
        if not self.validation_cache['predictions']:
            return {'status': 'no_validation_data'}
        
        calibration_metrics = self.measure_calibration()
        
        # Î³ ë¶„í¬ í†µê³„
        gamma_stats = {
            'mean': self.gamma_per_sample.mean().item(),
            'std': self.gamma_per_sample.std().item(),
            'min': self.gamma_per_sample.min().item(),
            'max': self.gamma_per_sample.max().item()
        }
        
        return {
            'status': 'calibrated',
            'ece': calibration_metrics['ece'],
            'gamma_statistics': gamma_stats,
            'bin_accuracies': calibration_metrics['bin_accuracies'].tolist(),
            'bin_counts': calibration_metrics['bin_counts'].tolist()
        }
    
    def reset_calibration_cache(self):
        """Validation cache ì´ˆê¸°í™”"""
        self.validation_cache = {
            'predictions': [],
            'targets': [],
            'confidences': []
        }
        print("ğŸ”„ Calibration cache ì´ˆê¸°í™” ì™„ë£Œ")
    
    def reset_parameters(self):
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        self.gamma_per_sample.fill_(self.base_gamma)
        self.gamma_momentum.fill_(self.base_gamma)
        self.calibration_history.fill_(0)
        self.sample_count_history.fill_(0)
        self.sample_difficulties.fill_(0)
        print("ğŸ”„ AdaFocal Loss íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” ì™„ë£Œ")


class AdaFocalDL(TabularNeuralNetTorchModel):
    """
    AdaFocal Deep Learning model for AutoGluon
    
    This model integrates AdaFocal loss with AutoGluon's TabularNeuralNetTorchModel.
    
    Reference:
    - Paper: https://proceedings.neurips.cc/paper_files/paper/2022/file/0a692a24dbc744fca340b9ba33bc6522-Paper-Conference.pdf
    - Code: https://github.com/3mcloud/adafocal
    
    í•œê¸€ ì„¤ëª…:
    AutoGluonì„ ìœ„í•œ AdaFocal ë”¥ëŸ¬ë‹ ëª¨ë¸ì…ë‹ˆë‹¤.
    AdaFocal lossë¥¼ AutoGluonì˜ TabularNeuralNetTorchModelê³¼ í†µí•©í•©ë‹ˆë‹¤.
    """
    
    # AutoGluon ëª¨ë¸ ë“±ë¡ì„ ìœ„í•œ ê³ ìœ  ì‹ë³„ìë“¤
    ag_key = 'ADAFOCAL_DL'
    ag_name = 'ADAFOCAL_DL'
    ag_priority = 100
    _model_name = "AdaFocalDL"
    _model_type = "adafocal_dl_model"
    _typestr = "adafocal_dl_model_v1_adafocalloss"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™” (CustomFocalDLModelê³¼ ë™ì¼í•œ íŒ¨í„´)
        # AdaFocal Loss íŠ¹í™” íŒŒë¼ë¯¸í„°ë“¤
        self.base_gamma = 2.0
        self.momentum = 0.9
        self.num_bins = 15
        self.calibration_threshold = 0.2
        self.lambda_param = 1.0
        self.gamma_max = 20.0
        self.gamma_min = -2.0
        self.alpha = 1.0
        
        # LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ (CustomFocalDLModelê³¼ ë™ì¼!)
        self.lr_scheduler = True
        self.scheduler_type = 'cosine'  # scheduler_type ì¶”ê°€
        self.lr_scheduler_step_size = 50
        self.lr_scheduler_gamma = 0.1
        self.lr_scheduler_min_lr = 1e-6
        
        # ê¸°íƒ€ ê³µí†µ íŒŒë¼ë¯¸í„°ë“¤ (CustomFocalDLModelê³¼ ë™ì¼!)
        self.learning_rate = 3e-4
        self.weight_decay = 1e-5
        self.dropout_prob = 0.1
        self.hidden_size = 128
        self.num_layers = 2
        self.batch_size = 256
        self.max_epochs = 100
        self.patience = 10
        
        # Validation data cache
        # ê²€ì¦ ë°ì´í„° ìºì‹œ
        self.validation_data = None
        self.validation_targets = None
    
    def _set_params(self, **kwargs):
        """Set model parameters (ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •) - CustomFocalDLModelê³¼ ì™„ì „ ë™ì¼í•˜ê²Œ ë§ì¶¤!"""
        print(f"ğŸ”§ AdaFocalDL _set_params í˜¸ì¶œë¨! kwargs={kwargs}")
        
        # AdaFocal Loss íŠ¹í™” íŒŒë¼ë¯¸í„°ë“¤
        self.base_gamma = kwargs.pop('base_gamma', 2.0)
        self.momentum = kwargs.pop('momentum', 0.9)
        self.num_bins = kwargs.pop('num_bins', 15)
        self.calibration_threshold = kwargs.pop('calibration_threshold', 0.2)
        self.lambda_param = kwargs.pop('lambda_param', 1.0)
        self.gamma_max = kwargs.pop('gamma_max', 20.0)
        self.gamma_min = kwargs.pop('gamma_min', -2.0)
        self.alpha = kwargs.pop('alpha', 1.0)
        
        # LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ (CustomFocalDLModelê³¼ ë™ì¼!)
        self.lr_scheduler = kwargs.pop('lr_scheduler', True)
        self.scheduler_type = kwargs.pop('scheduler_type', 'cosine')  # scheduler_type ì¶”ê°€
        self.lr_scheduler_step_size = kwargs.pop('lr_scheduler_step_size', 50)
        self.lr_scheduler_gamma = kwargs.pop('lr_scheduler_gamma', 0.1)
        self.lr_scheduler_min_lr = kwargs.pop('lr_scheduler_min_lr', 1e-6)
        
        # ê¸°íƒ€ ê³µí†µ íŒŒë¼ë¯¸í„°ë“¤ (CustomFocalDLModelê³¼ ë™ì¼!)
        self.learning_rate = kwargs.pop('learning_rate', 3e-4)
        self.weight_decay = kwargs.pop('weight_decay', 1e-5)
        self.dropout_prob = kwargs.pop('dropout_prob', 0.1)
        self.hidden_size = kwargs.pop('hidden_size', 128)
        self.num_layers = kwargs.pop('num_layers', 2)
        self.batch_size = kwargs.pop('batch_size', 256)
        self.max_epochs = kwargs.pop('max_epochs', 100)
        self.patience = kwargs.pop('patience', 10)
        
        print(f"ğŸ”§ AdaFocalDL _set_params: base_gamma={self.base_gamma}, momentum={self.momentum}")
        print(f"ğŸ”§ AdaFocalDL _set_params: lr_scheduler={self.lr_scheduler}, lr_scheduler_step_size={self.lr_scheduler_step_size}")
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ë¡œ ì „ë‹¬ (CustomFocalDLModelê³¼ ë™ì¼!)
        return super()._set_params(**kwargs)
    
    def _get_default_loss_function(self):
        """Get default loss function (ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°) - AdaFocal Loss ì‚¬ìš©"""
        # num_classesëŠ” ê¸°ë³¸ê°’ 2ë¡œ ì„¤ì • (CustomFocalDLModelê³¼ ë™ì¼í•œ íŒ¨í„´)
        num_classes = getattr(self, 'num_classes', 2)
        
        return AdaFocalLoss(
            num_classes=num_classes,
            alpha=self.alpha,  # self.alpha ì‚¬ìš©
            base_gamma=self.base_gamma,
            momentum=self.momentum,
            num_bins=self.num_bins,
            calibration_threshold=self.calibration_threshold,
            lambda_param=self.lambda_param,
            gamma_max=self.gamma_max,
            gamma_min=self.gamma_min
        )
    
    def _get_net(self, train_dataset, params):
        """Get neural network architecture (ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ê°€ì ¸ì˜¤ê¸°) - CustomFocalDLModelê³¼ ì™„ì „ ë™ì¼í•˜ê²Œ ë§ì¶¤!"""
        print(f"ğŸ”§ AdaFocalDL _get_net í˜¸ì¶œë¨! params={params}")
        
        # AdaFocal Loss íŠ¹í™” íŒŒë¼ë¯¸í„°ë“¤ì„ í•„í„°ë§í•˜ê³  selfì— ì €ì¥
        filtered_params = params.copy()
        self.base_gamma = filtered_params.pop('base_gamma', 2.0)
        self.momentum = filtered_params.pop('momentum', 0.9)
        self.num_bins = filtered_params.pop('num_bins', 15)
        self.calibration_threshold = filtered_params.pop('calibration_threshold', 0.2)
        self.lambda_param = filtered_params.pop('lambda_param', 1.0)
        self.gamma_max = filtered_params.pop('gamma_max', 20.0)
        self.gamma_min = filtered_params.pop('gamma_min', -2.0)
        self.alpha = filtered_params.pop('alpha', 1.0)
        
        # LR scheduler ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ì„ í•„í„°ë§
        self.lr_scheduler = filtered_params.pop('lr_scheduler', True)
        self.scheduler_type = filtered_params.pop('scheduler_type', 'cosine')  # scheduler_type ì¶”ê°€
        self.lr_scheduler_step_size = filtered_params.pop('lr_scheduler_step_size', 50)
        self.lr_scheduler_gamma = filtered_params.pop('lr_scheduler_gamma', 0.1)
        self.lr_scheduler_min_lr = filtered_params.pop('lr_scheduler_min_lr', 1e-6)
        
        # ê¸°íƒ€ ê³µí†µ íŒŒë¼ë¯¸í„°ë“¤ì„ í•„í„°ë§
        self.learning_rate = filtered_params.pop('learning_rate', 3e-4)
        self.weight_decay = filtered_params.pop('weight_decay', 1e-5)
        self.dropout_prob = filtered_params.pop('dropout_prob', 0.1)
        self.hidden_size = filtered_params.pop('hidden_size', 128)
        self.num_layers = filtered_params.pop('num_layers', 2)
        self.batch_size = filtered_params.pop('batch_size', 256)
        self.max_epochs = filtered_params.pop('max_epochs', 100)
        self.patience = filtered_params.pop('patience', 10)
        
        print(f"ğŸ”§ AdaFocalDL _get_net: base_gamma={self.base_gamma}, momentum={self.momentum}")
        print(f"ğŸ”§ AdaFocalDL _get_net: lr_scheduler={self.lr_scheduler}, lr_scheduler_step_size={self.lr_scheduler_step_size}")
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ë¡œ EmbedNet ìƒì„± (CustomFocalDLModelê³¼ ë™ì¼!)
        return super()._get_net(train_dataset, filtered_params)
    
    # _get_optimizerì™€ _get_schedulerëŠ” ì œê±° - TabularNeuralNetTorchModelì˜ ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©
    # CustomFocalDLModelê³¼ ë™ì¼í•˜ê²Œ ë§ì¶¤!
    
    def _train_net(
        self,
        train_dataset,
        loss_kwargs,
        batch_size,
        num_epochs,
        epochs_wo_improve,
        val_dataset=None,
        test_dataset=None,
        time_limit=None,
        reporter=None,
        verbosity=2,
    ):
        """AutoGluonì˜ _train_net ë©”ì„œë“œë¥¼ ë³µì‚¬í•˜ê³  LR scheduler ì¶”ê°€ - CustomFocalDLModelê³¼ ë™ì¼í•˜ê²Œ ë§ì¶¤!"""
        print("ğŸš€ AdaFocalDL _train_net í˜¸ì¶œë¨!")  # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€
        import torch
        import torch.optim.lr_scheduler as lr_scheduler
        import time
        import logging
        import io
        from copy import deepcopy

        start_time = time.time()
        logging.debug("initializing neural network...")
        self.model.init_params()
        logging.debug("initialized")
        train_dataloader = train_dataset.build_loader(batch_size, self.num_dataloading_workers, is_test=False)

        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get("loss_function", "auto") == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()
        
        # LR scheduler ì„¤ì • (optimizer ìƒì„± í›„)
        scheduler = None
        print(f"ğŸ” AdaFocalDL Debug: self.lr_scheduler = {getattr(self, 'lr_scheduler', 'NOT_SET')}")
        print(f"ğŸ” AdaFocalDL Debug: self.scheduler_type = {getattr(self, 'scheduler_type', 'NOT_SET')}")
        print(f"ğŸ” AdaFocalDL Debug: hasattr(self, 'lr_scheduler') = {hasattr(self, 'lr_scheduler')}")
        print(f"ğŸ” AdaFocalDL Debug: self.lr_scheduler (if exists) = {self.lr_scheduler if hasattr(self, 'lr_scheduler') else 'N/A'}")
        
        # ìˆ˜ì •: _set_paramsì—ì„œ ì €ì¥í•œ LR ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„° ì§ì ‘ ì‚¬ìš©
        if hasattr(self, 'lr_scheduler') and self.lr_scheduler:
            if hasattr(self, 'scheduler_type') and self.scheduler_type == 'cosine':
                scheduler = lr_scheduler.CosineAnnealingLR(
                    self.optimizer,
                    T_max=num_epochs,
                    eta_min=self.lr_scheduler_min_lr
                )
                print(f"âœ… AdaFocalDL: Cosine Annealing LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨ (min_lr={self.lr_scheduler_min_lr})")
            else:
                scheduler = lr_scheduler.ReduceLROnPlateau(
                    self.optimizer,
                    mode='max',
                    factor=0.2,
                    patience=5,
                    min_lr=self.lr_scheduler_min_lr
                )
                print(f"âœ… AdaFocalDL: ReduceLROnPlateau LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨")
        else:
            print(f"âŒ AdaFocalDL: LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        # DCNv2ì™€ ë™ì¼í•œ í˜•ì‹ì˜ ìƒì„¸í•œ í•™ìŠµ ë£¨í”„ í˜¸ì¶œ
        self._train_with_scheduler(train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity, scheduler)
    
    def _train_with_scheduler(
        self,
        train_dataset,
        loss_kwargs,
        batch_size,
        num_epochs,
        epochs_wo_improve,
        val_dataset=None,
        test_dataset=None,
        time_limit=None,
        reporter=None,
        verbosity=2,
        scheduler=None,
    ):
        """ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í•¨ê»˜ í•™ìŠµí•˜ë©´ì„œ LR ëª¨ë‹ˆí„°ë§"""
        import torch
        import time
        import io

        # ê¸°ë³¸ ì„¤ì •
        self.model.init_params()
        train_dataloader = train_dataset.build_loader(
            batch_size, self.num_dataloading_workers, is_test=False
        )

        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get(
            "loss_function", "auto"
        ) == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()

        # Early stopping ì„¤ì •
        if epochs_wo_improve is not None:
            early_stopping_method = self._get_early_stopping_strategy(
                num_rows_train=len(train_dataset)
            )
        else:
            early_stopping_method = self._get_early_stopping_strategy(
                num_rows_train=len(train_dataset)
            )

        # Validation ë¼ë²¨
        if val_dataset is not None:
            y_val = val_dataset.get_labels()
            if y_val.ndim == 2 and y_val.shape[1] == 1:
                y_val = y_val.flatten()
        else:
            y_val = None

        # ëª¨ë¸ ì €ì¥ìš© ë²„í¼
        io_buffer = None
        best_epoch = 0
        best_val_metric = -float("inf")

        # í•™ìŠµ ë£¨í”„
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            num_batches = 0

            # í˜„ì¬ LR ì¶œë ¥
            current_lr = self.optimizer.param_groups[0]["lr"]

            # ë°°ì¹˜ë³„ í•™ìŠµ
            for batch_idx, data_batch in enumerate(train_dataloader):
                self.optimizer.zero_grad()
                loss = self.model.compute_loss(data_batch, **loss_kwargs)
                loss.backward()

                # Gradient clipping (NaN ë°©ì§€)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                total_loss += loss.item()
                num_batches += 1

            # ì—í¬í¬ í‰ê·  ì†ì‹¤
            avg_loss = total_loss / num_batches
            epoch_time = time.time() - epoch_start_time

            # ---------------------------
            # Validation í‰ê°€ (f1 + loss)
            # ---------------------------
            val_metric = None
            val_loss = None
            if val_dataset is not None:
                # f1 (AutoGluon eval_metric ê¸°ë°˜)
                val_metric = self.score(
                    X=val_dataset, y=y_val, metric=self.stopping_metric, _reset_threads=False
                )

                # validation loss (scheduler ìš©)
                with torch.no_grad():
                    val_loss_total = 0.0
                    val_batches = 0
                    for data_batch in val_dataset.build_loader(
                        batch_size, self.num_dataloading_workers, is_test=True
                    ):
                        loss = self.model.compute_loss(data_batch, **loss_kwargs)
                        val_loss_total += loss.item()
                        val_batches += 1
                    val_loss = val_loss_total / max(1, val_batches)

                # Best model ì €ì¥ (f1 ê¸°ì¤€)
                if val_metric > best_val_metric:
                    best_val_metric = val_metric
                    io_buffer = io.BytesIO()
                    torch.save(self.model, io_buffer)
                    best_epoch = epoch + 1

            # ---------------------------
            # ë¡œê·¸ ì¶œë ¥
            # ---------------------------
            import time as time_module

            current_time = time_module.strftime("%Y-%m-%d %H:%M:%S")
            if val_metric is not None:
                log_msg = (
                    f"[{current_time}] Epoch {epoch+1}/{num_epochs}: "
                    f"Train loss: {avg_loss:.4f}, "
                    f"Val {self.stopping_metric.name}: {val_metric:.4f}, "
                    f"Val loss: {val_loss:.4f}, "
                    f"LR: {current_lr:.2e}, "
                    f"Best Epoch: {best_epoch}, "
                    f"Time: {epoch_time:.2f}s"
                )
            else:
                log_msg = (
                    f"[{current_time}] Epoch {epoch+1}/{num_epochs}: "
                    f"Train loss: {avg_loss:.4f}, "
                    f"LR: {current_lr:.2e}, "
                    f"Time: {epoch_time:.2f}s"
                )
            print(log_msg)

            # ---------------------------
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            # ---------------------------
            if scheduler:
                if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingLR):
                    scheduler.step()
                elif isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    # âœ… Validation loss ê¸°ì¤€ (í‘œì¤€ì ì¸ ì ‘ê·¼)
                    scheduler.step(val_loss if val_loss is not None else avg_loss)

            # ---------------------------
            # Early stopping ì²´í¬ (f1 ê¸°ì¤€)
            # ---------------------------
            if val_dataset is not None:
                is_best = val_metric > best_val_metric
                early_stop = early_stopping_method.update(
                    cur_round=epoch, is_best=is_best
                )
                if early_stop:
                    print(f"   Early stopping at epoch {epoch+1}")
                    break

        # ---------------------------
        # Best model ë¡œë“œ
        # ---------------------------
        if io_buffer is not None:
            io_buffer.seek(0)
            self.model = torch.load(io_buffer, weights_only=False)
            print(
                f"   Best model loaded from epoch {best_epoch} "
                f"(Val {self.stopping_metric.name}: {best_val_metric:.4f})"
            )

        # save trained parameters
        self.params_trained["batch_size"] = batch_size
        self.params_trained["num_epochs"] = best_epoch 
    
    # _get_default_ag_argsëŠ” ì œê±° - TabularNeuralNetTorchModelì˜ ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©
    # CustomFocalDLModelê³¼ ë™ì¼í•˜ê²Œ ë§ì¶¤!
    
    # _get_default_resourcesëŠ” ì œê±° - TabularNeuralNetTorchModelì˜ ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©
    # CustomFocalDLModelê³¼ ë™ì¼í•˜ê²Œ ë§ì¶¤!
    
    def update_validation_cache(self, predictions: torch.Tensor, targets: torch.Tensor):
        """Validation cache ì—…ë°ì´íŠ¸ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥)"""
        loss_function = self._get_default_loss_function()
        if hasattr(loss_function, 'update_validation_cache'):
            loss_function.update_validation_cache(predictions, targets)
    
    def get_calibration_summary(self) -> Dict[str, any]:
        """Calibration ìƒíƒœ ìš”ì•½ ë°˜í™˜ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥)"""
        loss_function = self._get_default_loss_function()
        if hasattr(loss_function, 'get_calibration_summary'):
            return loss_function.get_calibration_summary()
        return {'status': 'not_supported'}
    
    def reset_calibration_cache(self):
        """Calibration cache ì´ˆê¸°í™” (ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥)"""
        loss_function = self._get_default_loss_function()
        if hasattr(loss_function, 'reset_calibration_cache'):
            loss_function.reset_calibration_cache()


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ”§ AdaFocal Loss í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ìƒì„±
    model = AdaFocalDL()
    
    # íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        'focal_alpha': 1.0,
        'focal_base_gamma': 2.0,
        'focal_momentum': 0.9,
        'focal_num_bins': 10,
        'focal_calibration_threshold': 0.1
    }
    
    # íŒŒë¼ë¯¸í„° ì ìš©
    model._set_params(**params)
    
    print("âœ… AdaFocal Loss í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 