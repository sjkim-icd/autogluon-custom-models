import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import optuna
from autogluon.tabular import TabularPredictor
from autogluon.tabular.registry import ag_model_registry
from custom_models.tabular_dcnv2_torch_model import TabularDCNv2TorchModel
from custom_models.tabular_dcnv2_fuxictr_torch_model_fixed import TabularDCNv2FuxiCTRTorchModel
from custom_models.focal_loss_implementation import CustomFocalDLModel
from custom_models.custom_nn_torch_model import CustomNNTorchModel
from custom_models.advanced_focal_loss import AdvancedFocalDLModel
from custom_models.adafocal_loss import AdaFocalDL  # AdaFocal ëª¨ë¸ ì¶”ê°€

from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
import time
import json
from datetime import datetime
import argparse

# ëª¨ë¸ ë“±ë¡
ag_model_registry.add(TabularDCNv2TorchModel)
ag_model_registry.add(TabularDCNv2FuxiCTRTorchModel)
ag_model_registry.add(CustomFocalDLModel)
ag_model_registry.add(CustomNNTorchModel)
ag_model_registry.add(AdvancedFocalDLModel)
ag_model_registry.add(AdaFocalDL)  # AdaFocal ëª¨ë¸ ë“±ë¡

def load_titanic_data():
    """Titanic ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“Š Titanic ë°ì´í„° ë¡œë“œ ì¤‘...")
    titanic = fetch_openml("titanic", version=1, as_frame=True)
    df = titanic.frame
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    df = df.dropna(subset=['survived'])
    df['survived'] = df['survived'].astype(int)
    
    # ë°ì´í„° ë¶„í• 
    train_data, test_data = train_test_split(
        df,
        test_size=0.2,
        random_state=42,
        stratify=df['survived']
    )
    
    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}ê°œ")
    
    return train_data, test_data

def load_credit_card_data():
    """Credit Card ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    print(" Credit Card ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv("datasets/creditcard.csv")
    
    print(f"ğŸ“ˆ ë°ì´í„° í˜•íƒœ: {df.shape}")
    print(f" íƒ€ê²Ÿ ë¶„í¬:")
    print(df['Class'].value_counts())
    
    # ë°ì´í„° ë¶„í• 
    train_data, test_data = train_test_split(
        df, test_size=0.2, random_state=42, 
        stratify=df['Class']
    )
    
    print(f"âœ… í•™ìŠµ ë°ì´í„°: {train_data.shape[0]}ê°œ")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_data.shape[0]}ê°œ")
    
    return train_data, test_data

def load_data(dataset_name):
    """ë°ì´í„°ì…‹ ì„ íƒì— ë”°ë¥¸ ë°ì´í„° ë¡œë“œ"""
    if dataset_name.lower() == 'titanic':
        return load_titanic_data()
    elif dataset_name.lower() == 'creditcard':
        return load_credit_card_data()
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_name}")

def individual_model_hpo(train_data, test_data, experiment_name):
    """ê°œë³„ ëª¨ë¸ ìµœì í™” (í†µí•© DB ì‚¬ìš©)"""
    print("=== ê°œë³„ ëª¨ë¸ ìµœì í™” (í†µí•© DB) ===")
    
    best_params = {}
    model_types = ['DCNV2', 'DCNV2_FUXICTR', 'CUSTOM_FOCAL_DL', 'CUSTOM_NN_TORCH', 'ADVANCED_FOCAL_DL', 'ADAFOCAL_DL', 'RF']  # 7ê°œ ëª¨ë¸ë¡œ í™•ì¥
    # model_types = [ 'ADAFOCAL_DL']  # 7ê°œ ëª¨ë¸ë¡œ í™•ì¥
    
    # ì‹¤í—˜ë³„ DB ê²½ë¡œ ìë™ êµ¬ì„±
    db_dir = f'optuna_studies/{experiment_name}'
    os.makedirs(db_dir, exist_ok=True)
    unified_db_path = f'sqlite:///{db_dir}/all_studies.db'
    
    print(f"ğŸ” DB ê²½ë¡œ: {db_dir}/all_studies.db")
    
    for model_type in model_types:
        print(f"\nğŸ” {model_type} ìµœì í™” ì‹œì‘")
        print("=" * 50)
        
        # Optuna study ìƒì„± (í†µí•© DB ì‚¬ìš©)
        study = optuna.create_study(
            direction='maximize',
            study_name=f'{model_type}_hpo_study',
            storage=unified_db_path,  # í•˜ë‚˜ì˜ DB íŒŒì¼ ì‚¬ìš©
            load_if_exists=True
        )
        
        # Objective í•¨ìˆ˜ ìƒì„±
        def objective(trial):
            # ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
            if model_type == 'DCNV2':
                params = {
                    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
                    'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                    'dropout_prob': trial.suggest_categorical('dropout_prob', [0.1, 0.2, 0.3]),
                    'num_layers': trial.suggest_categorical('num_layers', [3, 4, 5]),
                    'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),
                    'num_epochs': trial.suggest_categorical('num_epochs', [15, 20, 25]),
                }
            elif model_type == 'DCNV2_FUXICTR':
                params = {
                    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
                    'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                    'dropout_prob': trial.suggest_categorical('dropout_prob', [0.1, 0.2, 0.3]),
                    'num_layers': trial.suggest_categorical('num_layers', [3, 4, 5]),
                    'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),
                    'num_epochs': trial.suggest_categorical('num_epochs', [15, 20, 25]),
                }
            elif model_type == 'CUSTOM_FOCAL_DL':
                params = {
                    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
                    'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                    'dropout_prob': trial.suggest_categorical('dropout_prob', [0.1, 0.2, 0.3]),
                    'num_layers': trial.suggest_categorical('num_layers', [3, 4, 5]),
                    'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),
                    'num_epochs': trial.suggest_categorical('num_epochs', [15, 20, 25]),
                    'focal_alpha': trial.suggest_categorical('focal_alpha', [0.25, 0.5, 0.75, 1.0]),
                    'focal_gamma': trial.suggest_categorical('focal_gamma', [1.0, 2.0, 3.0]),
                }
            elif model_type == 'CUSTOM_NN_TORCH':
                params = {
                    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
                    'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                    'dropout_prob': trial.suggest_categorical('dropout_prob', [0.1, 0.2, 0.3]),
                    'num_layers': trial.suggest_categorical('num_layers', [3, 4, 5]),
                    'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),
                    'num_epochs': trial.suggest_categorical('num_epochs', [15, 20, 25]),
                }
            elif model_type == 'ADVANCED_FOCAL_DL':
                params = {
                    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
                    'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                    'dropout_prob': trial.suggest_categorical('dropout_prob', [0.1, 0.2, 0.3]),
                    'num_layers': trial.suggest_categorical('num_layers', [3, 4, 5]),
                    'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),
                    'num_epochs': trial.suggest_categorical('num_epochs', [15, 20, 25]),
                    'focal_alpha': trial.suggest_categorical('focal_alpha', [0.25, 0.5, 0.75, 1.0]),
                    'base_gamma': trial.suggest_categorical('base_gamma', [1.0, 1.5, 2.0, 2.5, 3.0]),
                }
            elif model_type == 'ADAFOCAL_DL':  # AdaFocal ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€
                params = {
                    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
                    'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                    'dropout_prob': trial.suggest_categorical('dropout_prob', [0.1, 0.2, 0.3]),
                    'num_layers': trial.suggest_categorical('num_layers', [3, 4, 5]),
                    'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),
                    'num_epochs': trial.suggest_categorical('num_epochs', [15, 20, 25]),
                    'alpha': trial.suggest_categorical('alpha', [0.25, 0.5, 0.75, 1.0]),  # AdaFocal alpha
                    'base_gamma': trial.suggest_categorical('base_gamma', [1.0, 1.5, 2.0, 2.5, 3.0]),  # AdaFocal base_gamma
                    'momentum': trial.suggest_categorical('momentum', [0.8, 0.9, 0.95]),  # AdaFocal momentum
                    'num_bins': trial.suggest_categorical('num_bins', [10, 15, 20]),  # AdaFocal num_bins
                    'calibration_threshold': trial.suggest_categorical('calibration_threshold', [0.1, 0.2, 0.3]),  # AdaFocal Sth
                    'lambda_param': trial.suggest_categorical('lambda_param', [0.5, 1.0, 1.5]),  # AdaFocal Î»
                    'gamma_max': trial.suggest_categorical('gamma_max', [15.0, 20.0, 25.0]),  # AdaFocal Î³max
                    'gamma_min': trial.suggest_categorical('gamma_min', [-2.0, -1.5, -1.0]),  # AdaFocal Î³min
                }
            elif model_type == 'RF':
                params = {
                    'n_estimators': trial.suggest_categorical('n_estimators', [50, 100, 200]),
                    'max_depth': trial.suggest_categorical('max_depth', [5, 10, 15, None]),
                    'min_samples_split': trial.suggest_categorical('min_samples_split', [2, 5, 10]),
                    'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [1, 2, 4]),
                }
            
            # AutoGluon Predictor ì„¤ì •
            predictor = TabularPredictor(
                label='survived',
                problem_type='binary',
                eval_metric='f1',
                path=f"models/{model_type}_hpo",
                verbosity=0
            )
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
            hyperparameters = {
                model_type: params
            }
            
            try:
                # ëª¨ë¸ í•™ìŠµ
                predictor.fit(
                    train_data=train_data,
                    hyperparameters=hyperparameters,
                    time_limit=300,  # 5ë¶„ ì œí•œ
                    presets='medium_quality'
                )
                
                # ê²€ì¦ ì„±ëŠ¥ í‰ê°€
                val_scores = predictor.leaderboard()
                if len(val_scores) > 0:
                    best_score = val_scores.iloc[0]['score_val']
                    print(f"Trial {trial.number}: F1 = {best_score:.4f}")
                    return best_score
                else:
                    print(f"Trial {trial.number}: í•™ìŠµ ì‹¤íŒ¨")
                    return 0.0
                    
            except Exception as e:
                print(f"Trial {trial.number} ì˜¤ë¥˜: {e}")
                return 0.0
        
        # Optuna ìµœì í™” ì‹¤í–‰
        study.optimize(
            objective,
            n_trials=3, # ê° ëª¨ë¸ë‹¹ 15ë²ˆ
            timeout=600,   # 10ë¶„ ì œí•œ
            show_progress_bar=True
        )
        
        # ê²°ê³¼ ì €ì¥
        best_params[model_type] = {
            'best_value': study.best_value,
            'best_params': study.best_params
        }
        
        print(f"\nğŸ“Š {model_type} ìµœì í™” ê²°ê³¼:")
        print(f"ìµœê³  ì„±ëŠ¥: {study.best_value:.4f}")
        print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study.best_params}")
    
    return best_params

def final_ensemble_with_autogluon(train_data, test_data, best_params):
    """AutoGluon ìë™ ì•™ìƒë¸”ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ"""
    print("\n=== AutoGluon ìë™ ì•™ìƒë¸” í•™ìŠµ ===")
    
    # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì•™ìƒë¸” êµ¬ì„±
    hyperparameters = {}
    for model_type, result in best_params.items():
        hyperparameters[model_type] = result['best_params']
    
    print(f"ì•™ìƒë¸” êµ¬ì„±:")
    for model_type, params in hyperparameters.items():
        print(f"  {model_type}: {params}")
    
    # AutoGluon Predictor ì„¤ì •
    final_predictor = TabularPredictor(
        label='survived',
        problem_type='binary',
        eval_metric='f1',
        path="models/final_autogluon_ensemble",
        verbosity=4
    )
    
    # ëª¨ë“  ëª¨ë¸ì„ í•œë²ˆì— í•™ìŠµ (AutoGluonì´ ìë™ìœ¼ë¡œ ì•™ìƒë¸”)
    final_predictor.fit(
        train_data=train_data,
        hyperparameters=hyperparameters,
        time_limit=1200,  # 20ë¶„ ì œí•œ
        presets='medium_quality'
    )
    
    # ìµœì¢… ì„±ëŠ¥ í‰ê°€
    final_scores = final_predictor.leaderboard()
    print(f"\nğŸ† ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥:")
    print(final_scores)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
    test_predictions = final_predictor.predict(test_data)
    test_probabilities = final_predictor.predict_proba(test_data)
    
    from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
    
    test_f1 = f1_score(test_data['survived'], test_predictions)
    test_acc = accuracy_score(test_data['survived'], test_predictions)
    test_prec = precision_score(test_data['survived'], test_predictions)
    test_rec = recall_score(test_data['survived'], test_predictions)
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥:")
    print(f"F1 Score: {test_f1:.4f}")
    print(f"Accuracy: {test_acc:.4f}")
    print(f"Precision: {test_prec:.4f}")
    print(f"Recall: {test_rec:.4f}")
    
    return final_predictor, {
        'test_f1': test_f1,
        'test_acc': test_acc,
        'test_prec': test_prec,
        'test_rec': test_rec
    }

def run_single_stage_hpo_unified_db(experiment_name, dataset_name):  # dataset_name ì¸ì ì¶”ê°€
    """1ë‹¨ê³„ HPO + AutoGluon ìë™ ì•™ìƒë¸” ì‹¤í—˜ ì‹¤í–‰"""
    print(f"ğŸš€ 1ë‹¨ê³„ HPO + AutoGluon ìë™ ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘: {experiment_name}")
    
    # ë°ì´í„° ë¡œë“œ (ë°ì´í„°ì…‹ ì„ íƒ)
    train_data, test_data = load_data(dataset_name)  # dataset_name ì „ë‹¬
    
    # 1ë‹¨ê³„: ê°œë³„ ëª¨ë¸ ìµœì í™” (í†µí•© DB ì‚¬ìš©)
    best_params = individual_model_hpo(train_data, test_data, experiment_name)
    
    # ì‹¤í—˜ë³„ ê²°ê³¼ í´ë” ìƒì„±
    experiment_results_dir = f"results/{experiment_name}"
    os.makedirs(experiment_results_dir, exist_ok=True)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f"{experiment_results_dir}/best_individual_params_{experiment_name}_{timestamp}.json"
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(best_params, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ '{result_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # 2ë‹¨ê³„: AutoGluon ìë™ ì•™ìƒë¸”
    final_predictor, test_metrics = final_ensemble_with_autogluon(train_data, test_data, best_params)
    
    print(f"\nâœ… 1ë‹¨ê³„ HPO + AutoGluon ìë™ ì•™ìƒë¸” ì‹¤í—˜ ì™„ë£Œ (í†µí•© DB)!")
    print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ F1 Score: {test_metrics['test_f1']:.4f}")
    print(f"í†µí•© DB íŒŒì¼: optuna_studies/{experiment_name}/all_studies.db")
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë¶„ì„ ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print(f"python analysis/create_final_unified_dashboard_excel_fixed.py \"{experiment_name}\"")

def main():
    parser = argparse.ArgumentParser(description='Optuna HPO')
    parser.add_argument('experiment_name', type=str, help='ì‹¤í—˜ ì´ë¦„')
    parser.add_argument('dataset_name', type=str, choices=['titanic', 'creditcard'], help='ë°ì´í„°ì…‹ ì´ë¦„')  # ì¶”ê°€
    parser.add_argument('--num-trials', type=int, default=20, help='trial ìˆ˜')
    parser.add_argument('--timeout', type=int, default=3600, help='íƒ€ì„ì•„ì›ƒ(ì´ˆ)')
    
    args = parser.parse_args()
    
    experiment_name = args.experiment_name
    dataset_name = args.dataset_name  # ì¶”ê°€
    num_trials = args.num_trials
    timeout = args.timeout
    
      # ëª¨ë¸ íƒ€ì… ì •ì˜ - ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤!
    model_types = [
        'DCNV2', 'DCNV2_FUXICTR', 'CUSTOM_FOCAL_DL', 
        'CUSTOM_NN_TORCH', 'ADVANCED_FOCAL_DL', 'ADAFOCAL_DL', 'RF'
    ]

    print(f" ì‹¤í—˜ ì´ë¦„: {experiment_name}")
    print(f"ğŸ”„ Trial ìˆ˜: {num_trials}")
    print(f"â±ï¸  íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ")
    print(f"ğŸ“Š ì‹¤í—˜ êµ¬ì„±: {len(model_types)}ê°œ ëª¨ë¸ ({', '.join(model_types)})")
    print(f"ğŸ¯ ë°ì´í„°ì…‹: Titanic (ì´ì§„ ë¶„ë¥˜)")
    print(f"â±ï¸  ê° ëª¨ë¸ë‹¹ {num_trials} trials, ì´ {num_trials * len(model_types)} trials")
    
    # HPO ì‹¤í–‰
    run_single_stage_hpo_unified_db(args.experiment_name, args.dataset_name) 

if __name__ == "__main__":
    main() 