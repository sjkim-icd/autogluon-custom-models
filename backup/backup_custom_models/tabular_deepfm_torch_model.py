# [my_models] DeepFM ì§„ìž…ì 

from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel
from autogluon.common import space
from autogluon.tabular.models.tabular_nn.hyperparameters.parameters import get_default_param
from autogluon.tabular.models.tabular_nn.hyperparameters.searchspaces import get_default_searchspace
from custom_models.deepfm_block import DeepFMNet
import os

class TabularDeepFMTorchModel(TabularNeuralNetTorchModel):
    ag_key = "DEEPFM"         # â† ë°˜ë“œì‹œ ë¬¸ìžì—´, hyperparametersì—ì„œ ì‚¬ìš©í•  í‚¤
    ag_name = "DEEPFM"        # â† ì‚¬ëžŒì´ ì½ê¸° ì¢‹ì€ ì´ë¦„
    ag_priority = 100         # â† ìš°ì„ ìˆœìœ„(ì •ìˆ˜, ë†’ì„ìˆ˜ë¡ ë¨¼ì € í•™ìŠµ)
    _model_name = "TabularDeepFMTorchModel"
    _model_type = "tabular_deepfm_torch_model"
    _typestr = "tabular_deepfm_torch_model_v1_deepfm"  # âœ… ë°˜ë“œì‹œ NN_TORCHì™€ ë‹¤ë¥´ê²Œ

    def _set_default_params(self):
        """ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • - AutoGluon ê¸°ë³¸ + DeepFM ì»¤ìŠ¤í…€"""
        # AutoGluon ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        default_params = get_default_param(problem_type=self.problem_type, framework="pytorch")
        
        # DeepFM ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„° ì¶”ê°€
        deepfm_params = {
            'fm_dropout': 0.1,
            'fm_embedding_dim': 10,
            'deep_output_size': 128,
            'deep_hidden_size': 128,
            'deep_dropout': 0.1,
            'deep_layers': 3,
            # Learning Rate Scheduler ì„¤ì •
            'lr_scheduler': True,
            'scheduler_type': 'plateau',
            'lr_scheduler_patience': 5,
            'lr_scheduler_factor': 0.2,
            'lr_scheduler_min_lr': 1e-6,
        }
        default_params.update(deepfm_params)
        
        for param, val in default_params.items():
            self._set_default_param_value(param, val)

    def _get_default_searchspace(self):
        """DeepFM ëª¨ë¸ì˜ ê¸°ë³¸ ê²€ìƒ‰ ê³µê°„ ì •ì˜ - AutoGluon ê¸°ë³¸ + DeepFM ì»¤ìŠ¤í…€"""
        # AutoGluon ê¸°ë³¸ Search Space ê°€ì ¸ì˜¤ê¸°
        base_searchspace = get_default_searchspace(problem_type=self.problem_type, framework="pytorch")
        
        # DeepFM ì»¤ìŠ¤í…€ Search Space ì¶”ê°€
        if self.problem_type == 'binary':
            deepfm_searchspace = {
                'fm_dropout': space.Categorical(0.1, 0.2, 0.3),
                'fm_embedding_dim': space.Categorical(8, 10, 12),
                'deep_output_size': space.Categorical(64, 128, 256),
                'deep_hidden_size': space.Categorical(64, 128, 256),
                'deep_dropout': space.Categorical(0.1, 0.2, 0.3),
                'deep_layers': space.Categorical(2, 3, 4),
                # Learning Rate Scheduler Search Space
                'lr_scheduler': space.Categorical(True, False),
                'scheduler_type': space.Categorical('plateau', 'cosine', 'onecycle'),
                'lr_scheduler_patience': space.Categorical(3, 5, 7),
                'lr_scheduler_factor': space.Categorical(0.1, 0.2, 0.3),
                'lr_scheduler_min_lr': space.Real(1e-7, 1e-5, default=1e-6, log=True),
            }
        elif self.problem_type == 'multiclass':
            deepfm_searchspace = {
                'fm_dropout': space.Categorical(0.1, 0.2, 0.3),
                'fm_embedding_dim': space.Categorical(8, 10, 12),
                'deep_output_size': space.Categorical(64, 128, 256),
                'deep_hidden_size': space.Categorical(64, 128, 256),
                'deep_dropout': space.Categorical(0.1, 0.2, 0.3),
                'deep_layers': space.Categorical(2, 3, 4),
                # Learning Rate Scheduler Search Space
                'lr_scheduler': space.Categorical(True, False),
                'scheduler_type': space.Categorical('plateau', 'cosine', 'onecycle'),
                'lr_scheduler_patience': space.Categorical(3, 5, 7),
                'lr_scheduler_factor': space.Categorical(0.1, 0.2, 0.3),
                'lr_scheduler_min_lr': space.Real(1e-7, 1e-5, default=1e-6, log=True),
            }
        elif self.problem_type == 'regression':
            deepfm_searchspace = {
                'fm_dropout': space.Categorical(0.1, 0.2, 0.3),
                'fm_embedding_dim': space.Categorical(8, 10, 12),
                'deep_output_size': space.Categorical(64, 128, 256),
                'deep_hidden_size': space.Categorical(64, 128, 256),
                'deep_dropout': space.Categorical(0.1, 0.2, 0.3),
                'deep_layers': space.Categorical(2, 3, 4),
                # Learning Rate Scheduler Search Space
                'lr_scheduler': space.Categorical(True, False),
                'scheduler_type': space.Categorical('plateau', 'cosine', 'onecycle'),
                'lr_scheduler_patience': space.Categorical(3, 5, 7),
                'lr_scheduler_factor': space.Categorical(0.1, 0.2, 0.3),
                'lr_scheduler_min_lr': space.Real(1e-7, 1e-5, default=1e-6, log=True),
            }
        else:
            deepfm_searchspace = {}
        
        # ê¸°ë³¸ Search Spaceì™€ ì»¤ìŠ¤í…€ Search Space í•©ì¹˜ê¸°
        base_searchspace.update(deepfm_searchspace)
        return base_searchspace

    @classmethod
    def get_default_searchspace(cls, problem_type, num_classes=None, **kwargs):
        """DeepFM ëª¨ë¸ì˜ ê¸°ë³¸ ê²€ìƒ‰ ê³µê°„ ì •ì˜ (í´ëž˜ìŠ¤ ë©”ì„œë“œ - ì™¸ë¶€ í˜¸ì¶œìš©)"""
        # AutoGluon ê¸°ë³¸ Search Space ê°€ì ¸ì˜¤ê¸°
        base_searchspace = get_default_searchspace(problem_type=problem_type, framework="pytorch")
        
        # DeepFM ì»¤ìŠ¤í…€ Search Space ì¶”ê°€
        if problem_type == 'binary':
            deepfm_searchspace = {
                'fm_dropout': space.Categorical(0.1, 0.2, 0.3),
                'fm_embedding_dim': space.Categorical(8, 10, 12),
                'deep_output_size': space.Categorical(64, 128, 256),
                'deep_hidden_size': space.Categorical(64, 128, 256),
                'deep_dropout': space.Categorical(0.1, 0.2, 0.3),
                'deep_layers': space.Categorical(2, 3, 4),
            }
        elif problem_type == 'multiclass':
            deepfm_searchspace = {
                'fm_dropout': space.Categorical(0.1, 0.2, 0.3),
                'fm_embedding_dim': space.Categorical(8, 10, 12),
                'deep_output_size': space.Categorical(64, 128, 256),
                'deep_hidden_size': space.Categorical(64, 128, 256),
                'deep_dropout': space.Categorical(0.1, 0.2, 0.3),
                'deep_layers': space.Categorical(2, 3, 4),
            }
        elif problem_type == 'regression':
            deepfm_searchspace = {
                'fm_dropout': space.Categorical(0.1, 0.2, 0.3),
                'fm_embedding_dim': space.Categorical(8, 10, 12),
                'deep_output_size': space.Categorical(64, 128, 256),
                'deep_hidden_size': space.Categorical(64, 128, 256),
                'deep_dropout': space.Categorical(0.1, 0.2, 0.3),
                'deep_layers': space.Categorical(2, 3, 4),
            }
        else:
            deepfm_searchspace = {}
        
        # ê¸°ë³¸ Search Spaceì™€ ì»¤ìŠ¤í…€ Search Space í•©ì¹˜ê¸°
        base_searchspace.update(deepfm_searchspace)
        return base_searchspace

    def _get_net(self, train_dataset, params):
        # EmbedNet ëŒ€ì‹  DeepFMNet ìƒì„±
        params = self._set_net_defaults(train_dataset, params)
        
        # DeepFMNet ìƒì„±
        model = DeepFMNet(
            problem_type=self.problem_type,
            num_net_outputs=self._get_num_net_outputs(),
            quantile_levels=self.quantile_levels,
            train_dataset=train_dataset,
            device=self.device,
            **params,
        )
        model = model.to(self.device)
        
        # self.model ì„¤ì •
        self.model = model
        
        if not os.path.exists(self.path):
            os.makedirs(self.path)
        
        return model
    
    def _train_net(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset=None, test_dataset=None, time_limit=None, reporter=None, verbosity=2):
        """LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì ìš©ëœ ì»¤ìŠ¤í…€ í•™ìŠµ ë©”ì„œë“œ"""
        print("ðŸš€ DeepFM _train_net í˜¸ì¶œë¨!")  # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€
        import torch
        import torch.optim.lr_scheduler as lr_scheduler
        
        # LR ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        scheduler = None
        if hasattr(self.model, 'lr_scheduler') and self.model.lr_scheduler:
            if self.model.scheduler_type == 'cosine':
                scheduler = lr_scheduler.CosineAnnealingLR(
                    self.optimizer, 
                    T_max=num_epochs,
                    eta_min=self.model.lr_scheduler_min_lr
                )
                print(f"âœ… DeepFM: Cosine Annealing LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨ (min_lr={self.model.lr_scheduler_min_lr})")
            elif self.model.scheduler_type == 'plateau':
                scheduler = lr_scheduler.ReduceLROnPlateau(
                    self.optimizer,
                    mode='max',
                    factor=self.model.lr_scheduler_factor,
                    patience=self.model.lr_scheduler_patience,
                    min_lr=self.model.lr_scheduler_min_lr
                )
                print(f"âœ… DeepFM: ReduceLROnPlateau LR ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©ë¨ (factor={self.model.lr_scheduler_factor}, patience={self.model.lr_scheduler_patience})")
            else:
                print(f"âš ï¸ DeepFM: ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ìž… '{self.model.scheduler_type}'")
        
        # ë¶€ëª¨ í´ëž˜ìŠ¤ì˜ ê¸°ë³¸ í•™ìŠµ ë¡œì§ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í•¨ê»˜)
        if scheduler:
            # ì»¤ìŠ¤í…€ í•™ìŠµ ë£¨í”„ë¡œ LR ëª¨ë‹ˆí„°ë§
            self._train_with_scheduler(train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity, scheduler)
        else:
            # ê¸°ë³¸ í•™ìŠµ
            super()._train_net(train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity)
    
    def _train_with_scheduler(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset=None, test_dataset=None, time_limit=None, reporter=None, verbosity=2, scheduler=None):
        """ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í•¨ê»˜ í•™ìŠµí•˜ë©´ì„œ LR ëª¨ë‹ˆí„°ë§"""
        import torch
        import time
        import io
        
        # ê¸°ë³¸ ì„¤ì •
        self.model.init_params()
        train_dataloader = train_dataset.build_loader(batch_size, self.num_dataloading_workers, is_test=False)
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        if isinstance(loss_kwargs.get("loss_function", "auto"), str) and loss_kwargs.get("loss_function", "auto") == "auto":
            loss_kwargs["loss_function"] = self._get_default_loss_function()
        
        # Early stopping ì„¤ì •
        if epochs_wo_improve is not None:
            early_stopping_method = self._get_early_stopping_strategy(num_rows_train=len(train_dataset))
        else:
            early_stopping_method = self._get_early_stopping_strategy(num_rows_train=len(train_dataset))
        
        # Validation ë°ì´í„° ì¤€ë¹„
        if val_dataset is not None:
            y_val = val_dataset.get_labels()
            if y_val.ndim == 2 and y_val.shape[1] == 1:
                y_val = y_val.flatten()
        else:
            y_val = None
        
        # ëª¨ë¸ ì €ìž¥ìš© ë²„í¼
        io_buffer = None
        best_epoch = 0
        best_val_metric = -float('inf')
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            num_batches = 0
            
            # í˜„ìž¬ LR ì¶œë ¥
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ë°°ì¹˜ë³„ í•™ìŠµ
            for batch_idx, data_batch in enumerate(train_dataloader):
                self.optimizer.zero_grad()
                loss = self.model.compute_loss(data_batch, **loss_kwargs)
                loss.backward()
                
                # Gradient clipping ì¶”ê°€ (nan ë°©ì§€)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            # ì—í¬í¬ í‰ê·  ì†ì‹¤
            avg_loss = total_loss / num_batches
            epoch_time = time.time() - epoch_start_time
            
            # Validation í‰ê°€
            val_metric = None
            if val_dataset is not None:
                val_metric = self.score(X=val_dataset, y=y_val, metric=self.stopping_metric, _reset_threads=False)
                
                # Best model ì €ìž¥
                if val_metric > best_val_metric:
                    best_val_metric = val_metric
                    io_buffer = io.BytesIO()
                    torch.save(self.model, io_buffer)
                    best_epoch = epoch + 1
            
            # ì¶œë ¥ (AutoGluon í˜•ì‹ + LR)
            if val_metric is not None:
                print(f"Epoch {epoch+1} (Update {epoch+1}).\t"
                      f"Train loss: {avg_loss:.4f}, "
                      f"Val {self.stopping_metric.name}: {val_metric:.4f}, "
                      f"LR: {current_lr:.2e}, "
                      f"Best Epoch: {best_epoch}")
            else:
                print(f"Epoch {epoch+1} (Update {epoch+1}).\t"
                      f"Train loss: {avg_loss:.4f}, "
                      f"LR: {current_lr:.2e}")
            
            print(f"   Time: {epoch_time:.2f}s")
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            if scheduler:
                if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingLR):
                    scheduler.step()
                elif isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    # validation metric ì‚¬ìš©
                    scheduler.step(val_metric if val_metric is not None else avg_loss)
            
            # Early stopping ì²´í¬
            if val_dataset is not None:
                is_best = (val_metric > best_val_metric) if epoch > 0 else True
                early_stop = early_stopping_method.update(cur_round=epoch, is_best=is_best)
                if early_stop:
                    print(f"   Early stopping at epoch {epoch+1}")
                    break
        
        # Best model ë¡œë“œ
        if io_buffer is not None:
            io_buffer.seek(0)
            self.model = torch.load(io_buffer, weights_only=False)
            print(f"   Best model loaded from epoch {best_epoch} (Val f1: {best_val_metric:.4f})")