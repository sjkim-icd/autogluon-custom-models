print("=== DCNV2 vs CUSTOM 모델들 차이점 분석 ===")
print()

print("🔍 **DCNV2의 문제점들:**")
print("1. **복잡한 아키텍처**:")
print("   - Cross Network + Deep Network 조합")
print("   - Low-rank factorization (U * V^T)")
print("   - 더 많은 하이퍼파라미터 (cross_layers, low_rank, deep_layers 등)")
print("   - 복잡한 forward pass: cross_output + deep_output 조합")
print()

print("2. **하이퍼파라미터 민감성**:")
print("   - num_cross_layers: 2, 3, 4")
print("   - low_rank: 16, 32, 64")
print("   - cross_dropout: 0.0, 0.1, 0.2")
print("   - deep_layers: 2, 3, 4")
print("   - deep_hidden_size: 64, 128, 256")
print("   - 이 모든 조합이 20번 실험에서 최적화되기 어려움")
print()

print("3. **구현상의 차이점**:")
print("   - DCNV2: 커스텀 CrossNetwork 클래스 구현")
print("   - CUSTOM_FOCAL_DL: AutoGluon 기본 EmbedNet + FocalLoss")
print("   - CUSTOM_NN_TORCH: AutoGluon 기본 EmbedNet + CrossEntropy")
print()

print("🔍 **CUSTOM 모델들이 더 나은 이유:**")
print("1. **단순한 아키텍처**:")
print("   - AutoGluon의 검증된 EmbedNet 사용")
print("   - 단순한 MLP 구조")
print("   - 적은 하이퍼파라미터")
print()

print("2. **안정적인 학습**:")
print("   - FocalLoss: 불균형 데이터에 특화")
print("   - CrossEntropy: 표준 분류 손실 함수")
print("   - AutoGluon의 최적화된 학습 루프")
print()

print("3. **하이퍼파라미터 최적화**:")
print("   - 더 적은 하이퍼파라미터로 20번 실험에서 더 효과적")
print("   - AutoGluon의 기본 검색 공간이 더 안정적")
print()

print("🔍 **DCNV2가 논문과 다른 점:**")
print("1. **Low-rank factorization**:")
print("   - 논문: W = U * V^T로 파라미터 수 감소")
print("   - 구현: U와 V를 별도로 학습")
print("   - 하지만 실제로는 더 복잡한 구조")
print()

print("2. **Cross Network 구현**:")
print("   - 논문: x_{l+1} = x_0 * x_l^T * w_l + x_l + b_l")
print("   - 구현: Low-rank로 근사화")
print("   - 복잡도 증가로 학습 불안정성 증가")
print()

print("3. **Deep Network 조합**:")
print("   - 논문: Cross + Deep 네트워크 병렬 구조")
print("   - 구현: Cross(원본크기) + Deep(128차원) 조합")
print("   - 차원 불일치로 인한 학습 어려움")
print()

print("🎯 **결론:**")
print("DCNV2는 논문의 아이디어는 좋지만, 실제 구현에서 너무 복잡해서")
print("20번의 HPO 실험으로는 최적화하기 어려운 구조입니다.")
print("반면 CUSTOM 모델들은 AutoGluon의 검증된 구조를 기반으로")
print("더 안정적이고 효과적인 학습이 가능합니다.") 